{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core: Train Model\n",
    "Train a LULC classifier. Number of categories and remapping can be selected dynamically (eg 3-cat vs full 6-cat vs roads).\n",
    "\n",
    "Workflow sidesteps some problems with TensorFlow by simplifying the training and shifting some components—multiple epochs, callback functionality, validation, etc—to manual coding.\n",
    "\n",
    "Currently, calls for training in just two epochs, one fast and one slow. But smaller training datasets in particular may require additional epochs.\n",
    "\n",
    "Date: 2019-01-18  \n",
    "Author: Peter Kerins  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical, comprehensive imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#\n",
    "import os, sys\n",
    "import json\n",
    "import itertools, collections\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geojson\n",
    "import fiona\n",
    "import ogr, gdal\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "import h5py\n",
    "\n",
    "import descarteslabs as dl\n",
    "\n",
    "ULU_REPO = os.environ[\"ULU_REPO\"]\n",
    "if ULU_REPO not in sys.path:\n",
    "    sys.path.append(ULU_REPO+'/utils')\n",
    "    sys.path.append(ULU_REPO)\n",
    "print(sys.path)\n",
    "\n",
    "import util_descartes\n",
    "#import util_ml\n",
    "import util_rasters\n",
    "import util_vectors\n",
    "import util_workflow\n",
    "import util_chips\n",
    "import util_training\n",
    "import util_network\n",
    "import util_scoring\n",
    "from catalog_generator import CatalogGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set all user-defined variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root='/data/phase_iv/'\n",
    "\n",
    "resolution=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcatalog_name = 'kampala_2img'\n",
    "\n",
    "path_train = data_root+'models/'+subcatalog_name+'_train.csv'\n",
    "path_valid = data_root+'models/'+subcatalog_name+'_valid.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdown_system = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chips variables\n",
    "Only needed if selecting samples from master catalog, rather than loading subcatalog from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_new = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_level = None\n",
    "source = 's2'\n",
    "#image_suffix = 'E'\n",
    "\n",
    "s2_bands=['blue','green','red','nir','swir1','swir2','alpha']; s2_suffix='BGRNS1S2A'  # S2, Lx\n",
    "# s1_bands=['vv','vh']; s1_suffix='VVVH'  \n",
    "\n",
    "resampling='bilinear'\n",
    "processing = None\n",
    "\n",
    "label_suffix = 'aue'\n",
    "label_lot = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_locales = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_images = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place_images['hindupur']=['U', 'V', 'W', 'X', 'Y', 'Z'],[13]\n",
    "# place_images['singrauli']=['O','P','Q','R','S','T','U'],[38]\n",
    "# place_images['vijayawada']=['H','I'],[68]\n",
    "# place_images['jaipur']=['T','U','W','X','Y','Z'],[27, 72]\n",
    "# place_images['hyderabad']=['P','Q','R','S','T','U'],[10, 44, 46, 60, 79, 55, 60]\n",
    "# place_images['sitapur']=['Q','R','T','U','V'],[2, 27, 43]\n",
    "# place_images['kanpur']=['AH', 'AK', 'AL', 'AM', 'AN'],[6, 19, 57, 67]\n",
    "# place_images['belgaum']=['P','Q','R','S','T'],[13]\n",
    "# place_images['parbhani']=['T','V','W','X','Y','Z'],[10, 42, 54]\n",
    "# place_images['pune']=['P', 'Q', 'T', 'U', 'S'],[9, 54, 73]\n",
    "# place_images['ahmedabad']= ['Z', 'V', 'W', 'X', 'Y', 'AA'],[22, 25, 45, 65, 70]\n",
    "# place_images['malegaon']=  ['V', 'W', 'X', 'Y', 'Z'],[6]\n",
    "# place_images['kolkata'] =  ['M','N','O','P','Q','R'],[16, 90, 105,  195, 218]\n",
    "# place_images['mumbai']=['P','Q','R','S','U','V'],[24, 42, 73, 98, 99, 103, 123, 131, 133, 152, 160, 172]\n",
    "# place_images['coimbatore']=['Q','R','S'],[15, 21, 68, 74]\n",
    "# place_images['jalna']=['AV','AW','AX'],[12, 20, 31, 34, 42, 44, 65, 69, 73]\n",
    "# place_images['kozhikode']=['J','K','L'],[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place_images['dhaka']=['A','B','C'],[72]\n",
    "# place_images['saidpur']=['A','B','C'],[2, 21, 32, 39, 43, 47, 52]\n",
    "# place_images['rajshahi']=['A','B','C'],[17]\n",
    "# place_images['lahore']=['A','B','C'],[33, 70]\n",
    "# place_images['karachi']=['A','B','C'],[20, 29, 62]\n",
    "# place_images['sialkot']=['A','B','C'],[32, 53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place_images['gorgan']=['A','B','C'],[36, 59, 69]\n",
    "# place_images['qom']=['A','B','C'],[1]\n",
    "# place_images['tehran']=['A','B','C'],[28, 56, 76]\n",
    "# place_images['shymkent']=['A','B','C'],[62]\n",
    "# place_images['pokhara']=['A','B','C'],[25, 28, 31, 49, 51]\n",
    "# place_images['bukhara']=['A','B','C'],[61]\n",
    "# place_images['tashkent']=['A','B','C'],[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place_images['culiacan']=['A', 'B'],[1, 9, 29, 51, 61, 66, 71]\n",
    "# place_images['guadalajara']=['A', 'B'],[22, 47, 65]\n",
    "# place_images['leon']=['A', 'B'],[3, 16, 27, 36, 38, 46, 67]\n",
    "# place_images['mexico-city']=['A', 'B'],[0, 13, 57, 75, 112, 183, 198]\n",
    "# place_images['reynosa']=['A', 'B'],[25, 31, 40, 52, 61]\n",
    "# place_images['tijuana']=['A', 'B'],[9, 45, 49, 53]\n",
    "# place_images['merida']=['A', 'B'],[25, 55, 57]\n",
    "# place_images['monterrey']=['A', 'B'],[1]\n",
    "# place_images['tuxtla']=['A', 'B'],[3, 20, 58, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place_images['johannesburg']=['A', 'B',],[76, 125, 132, 178, 118, 166, 187]\n",
    "place_images['kampala']=['A', 'B','C','D',],[59, 23, 22]\n",
    "# place_images['kigali']=['A', 'B'],[29, 48]\n",
    "# place_images['addis-ababa']=['A', 'B'],[32, 44, 43, 66, 65]\n",
    "# place_images['port-elizabeth']=['A', 'B'],[44, 15, 20, 30]\n",
    "# place_images['arusha']=['A', 'B'],[0, 8]\n",
    "# place_images['nakuru']=['A', 'B'],[15, 33, 9, 52, 79]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample construction variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bands stuff outdated! needs to be reconciled with catalog filtering\n",
    "# will ignore for the moment since this is a bigger fix...\n",
    "# haven't done any examples yet incorporating additional chips beyond s2\n",
    "# into construction of a training sample\n",
    "bands_vir=s2_bands[:-1]\n",
    "bands_sar=None\n",
    "bands_ndvi=None\n",
    "bands_ndbi=None\n",
    "bands_osm=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be updated completely; bands stuff doesn't make sense right now\n",
    "stack_label, feature_count = util_workflow.build_stack_label(\n",
    "        bands_vir=bands_vir,\n",
    "        bands_sar=bands_sar,\n",
    "        bands_ndvi=bands_ndvi,\n",
    "        bands_ndbi=bands_ndbi,\n",
    "        bands_osm=bands_osm,)\n",
    "print(stack_label, feature_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model & training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = '6cat_kampala_2img'\n",
    "notes = 'just two images from kampala'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapping = None\n",
    "n_cats = 6\n",
    "categories=[0,1,2,3,4,5,]\n",
    "exclude_roads = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "balancing = None\n",
    "\n",
    "epochs_fast = 1\n",
    "epochs_slow = 1\n",
    "\n",
    "max_queue_size = 64\n",
    "workers = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify training & validation samples\n",
    "Construct subcatalogs containing all target training & validation samples, __or__ load them from file, according to variable `build_new`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option A: Construct subcatalogs by filtering master catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if build_new:\n",
    "    df = util_chips.load_catalog()\n",
    "    print(len(df.index))\n",
    "    \n",
    "    new_places = [\n",
    "        'dhaka',\n",
    "        'saidpur',\n",
    "        'rajshahi',\n",
    "        'lahore',\n",
    "        'karachi',\n",
    "        'sialkot',\n",
    "        'coimbatore',\n",
    "        'jalna',\n",
    "        'kozhikode',\n",
    "        'bukhara',\n",
    "        'gorgan',\n",
    "        'pokhara',\n",
    "        'qom',\n",
    "        'shymkent',\n",
    "        'tashkent',\n",
    "        'tehran',\n",
    "        'culiacan',\n",
    "        'guadalajara',\n",
    "        'leon',\n",
    "        'reynosa',\n",
    "        'tijuana',\n",
    "        'merida',\n",
    "        'monterrey',\n",
    "        'tuxtla',\n",
    "        'johannesburg',\n",
    "        'kampala',\n",
    "        'kigali',\n",
    "        'addis-ababa',\n",
    "        'port-elizabeth',\n",
    "        'arusha',\n",
    "        'nakuru',\n",
    "    ]\n",
    "    \n",
    "    included_places = list(set(new_places) & set(place_images.keys()))\n",
    "    \n",
    "    for place in included_places:\n",
    "#         print(place)\n",
    "        place_catalog_path = data_root+'chip_catalog_'+place+'.csv'\n",
    "        print(place_catalog_path)\n",
    "\n",
    "        df_place = pd.read_csv(place_catalog_path)\n",
    "        print('no of chips:', len(df_place))\n",
    "\n",
    "        df = df.append(df_place, ignore_index=True)\n",
    "        \n",
    "\n",
    "    mask = pd.Series(data=np.zeros(len(df.index),dtype='uint8'), index=range(len(df)), dtype='uint8')\n",
    "\n",
    "    for place,entry in place_images.items():\n",
    "        image_list = entry[0]\n",
    "        exclusion_list = entry[1]\n",
    "        if exclude_locales:\n",
    "            mask |= (df['city']==place) & (df.image.isin(image_list)) & (~df.locale.isin(exclusion_list))\n",
    "        else:\n",
    "            mask |= (df['city']==place) & (df.image.isin(image_list))\n",
    "\n",
    "    if exclude_roads:\n",
    "        mask &= (df['lulc']!=6)\n",
    "\n",
    "    # filter others according to specifications\n",
    "    mask &= (df['gt_type']==label_suffix)\n",
    "    mask &= (df['gt_lot']==int(label_lot))\n",
    "    mask &= (df['source']==source)\n",
    "    mask &= (df['resolution']==int(resolution))\n",
    "    mask &= (df['resampling']==resampling)\n",
    "    mask &= (df['processing']==str(processing).lower())\n",
    "\n",
    "    print(np.sum(mask))\n",
    "\n",
    "    df = df[mask]\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    len(df)\n",
    "\n",
    "\n",
    "\n",
    "    combined_place_locales = {}\n",
    "    for place in place_images:\n",
    "        place_locales_filename = data_root+'models/'+'locales_'+place+'.pkl'\n",
    "        with open(place_locales_filename, \"rb\") as f:\n",
    "            place_locales = pickle.load(f,encoding='latin1')\n",
    "    #         print(place_locales)\n",
    "        combined_place_locales.update(place_locales)\n",
    "    pprint(combined_place_locales)\n",
    "\n",
    "\n",
    "    df_t, df_v = util_chips.mask_locales(df, combined_place_locales)\n",
    "    print(len(df_t), len(df_v))\n",
    "\n",
    "    # save the datasets for future use\n",
    "    %time df_t.to_csv(path_train,index=False)\n",
    "    %time df_v.to_csv(path_valid,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option B: Load existing subcatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not build_new:\n",
    "    df_t = pd.read_csv(path_train, encoding='utf8')\n",
    "    df_v = pd.read_csv(path_valid, encoding='utf8')\n",
    "    print(len(df_t), len(df_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect selected samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train:')\n",
    "print(util_training.calc_category_counts(df_t,remapping=None), len(df_t))\n",
    "print('valid:')\n",
    "print(util_training.calc_category_counts(df_v,remapping=None), len(df_v))\n",
    "print()\n",
    "if build_new:\n",
    "    print('all:')\n",
    "    print(util_training.calc_category_counts(df,remapping=None), len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(df_t.groupby(['city','image']).size().reset_index().rename(columns={0:'count'}))\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate class weighting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_weights = util_training.generate_category_weights(df_t,remapping=remapping,log=False,mu=1.0,max_score=None)\n",
    "print(category_weights.items())\n",
    "weights = list(zip(*category_weights.items()))[1]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_weights_filename = data_root+'models/'+model_id+'_category_weights.pkl'\n",
    "\n",
    "if not os.path.exists(category_weights_filename):\n",
    "    pickle.dump(category_weights, open(category_weights_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use weights to create weighted categorical crossentropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = util_training.make_loss_function_wcc(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build convolutional neural network and prepare it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hardcoded params\n",
    "network=util_network.build_xmodel(input_shape=(17,17,6),output_nodes=n_cats,input_conv_block=True)\n",
    "util_network.compile_network(network, loss, LR=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct \"fast\" training with high learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sample \"generators\" (Keras _sequence_ objects) to serve samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_t = CatalogGenerator(df_t,remapping=remapping,look_window=window,batch_size=batch_size,one_hot=n_cats)\n",
    "generator_v = CatalogGenerator(df_v,remapping=remapping,look_window=window,batch_size=batch_size,one_hot=n_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train fast\n",
    "#history_fast = network.fit(X_train, Y_t_cat, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, Y_v_cat), shuffle=True,callbacks=callbacks)\n",
    "#docs: fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None,\n",
    "                    #class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
    "history_fast = network.fit_generator(generator_t, epochs=epochs_fast, callbacks=None, steps_per_epoch=generator_t.steps,\n",
    "                                    #validation_data=generator_v, validation_steps=generator_v.steps,\n",
    "                                    shuffle=True,use_multiprocessing=True,max_queue_size=max_queue_size,workers=workers,)\n",
    "\n",
    "# plt.plot(history_fast.history['val_acc'])\n",
    "# plt.show()\n",
    "# plt.plot(history_fast.history['val_loss'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_weights_path = data_root + 'models/' + model_id + '_weights_fast' + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fast_weights_path)\n",
    "network.save_weights(fast_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild model and conduct \"slow\" training with lower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network=util_network.build_xmodel(input_shape=(17,17,6),output_nodes=n_cats,input_conv_block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load trained weights and prepare network for additional training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_weights(fast_weights_path)\n",
    "util_network.compile_network(network, loss, LR=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_t.reset()\n",
    "generator_v.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_slow = network.fit_generator(generator_t, epochs=epochs_slow, callbacks=None, steps_per_epoch=generator_t.steps,\n",
    "                                    #validation_data=generator_v, validation_steps=generator_v.steps,\n",
    "                                    shuffle=True,use_multiprocessing=True,max_queue_size=max_queue_size,workers=workers,)\n",
    "\n",
    "# plt.plot(history_slow.history['val_acc'])\n",
    "# plt.show()\n",
    "# plt.plot(history_slow.history['val_loss'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store further trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_weights_path = data_root + 'models/' + model_id + '_weights_slow' + '.h5'\n",
    "print(slow_weights_path)\n",
    "network.save_weights(slow_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store entire network object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_filename = data_root+'models/'+model_id+'.hd5'\n",
    "\n",
    "if os.path.exists(network_filename):\n",
    "    print('Cannot save network: file already exists at specified path ('+network_filename+')')\n",
    "else:\n",
    "    network.save(network_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model to training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_t.reset()\n",
    "#predict_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
    "predictions_t = network.predict_generator(generator_t, steps=generator_t.steps, verbose=1,\n",
    "                  use_multiprocessing=True,max_queue_size=max_queue_size,workers=workers,)\n",
    "print(predictions_t.shape)\n",
    "\n",
    "generator_v.reset()\n",
    "#predict_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
    "predictions_v = network.predict_generator(generator_v, steps=generator_v.steps, verbose=1,\n",
    "                  use_multiprocessing=True,max_queue_size=max_queue_size,workers=workers,)\n",
    "print(predictions_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat_t = predictions_t.argmax(axis=-1)\n",
    "print(Yhat_t.shape)\n",
    "Yhat_v = predictions_v.argmax(axis=-1)\n",
    "print(Yhat_v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract corresponding _actual_ ground-truth values directly from catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_t = generator_t.get_label_series().values\n",
    "print(Y_t.shape)\n",
    "Y_v = generator_v.get_label_series().values\n",
    "print(Y_v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate typical scoring information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"evaluate training\")\n",
    "# hardcoded categories\n",
    "categories=[0,1,2,3,4,5,6]\n",
    "train_confusion = util_scoring.calc_confusion(Yhat_t,Y_t,categories)\n",
    "train_recalls, train_precisions, train_accuracy = util_scoring.calc_confusion_details(train_confusion)\n",
    "\n",
    "# Calculate f-score\n",
    "beta = 2\n",
    "train_f_scores = (beta**2 + 1) * train_precisions * train_recalls / ( (beta**2 * train_precisions) + train_recalls )\n",
    "train_f_score_average = np.mean(train_f_scores)\n",
    "\n",
    "# expanding lists to match expected model_record stuff\n",
    "train_recalls_expanded = [None,None,None,None,None,None,None,]\n",
    "train_precisions_expanded = [None,None,None,None,None,None,None,]\n",
    "train_f_scores_expanded = [None,None,None,None,None,None,None,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"evaluate validation\")\n",
    "valid_confusion = util_scoring.calc_confusion(Yhat_v,Y_v,categories)\n",
    "valid_recalls, valid_precisions, valid_accuracy = util_scoring.calc_confusion_details(valid_confusion)\n",
    "\n",
    "# Calculate f-score\n",
    "beta = 2\n",
    "valid_f_scores = (beta**2 + 1) * valid_precisions * valid_recalls / ( (beta**2 * valid_precisions) + valid_recalls )\n",
    "valid_f_score_average = np.mean(valid_f_scores)\n",
    "\n",
    "# expanding lists to match expected model_record stuff\n",
    "valid_recalls_expanded = [None,None,None,None,None,None,None,]\n",
    "valid_precisions_expanded = [None,None,None,None,None,None,None,]\n",
    "valid_f_scores_expanded = [None,None,None,None,None,None,None,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(0,len(train_recalls)):\n",
    "    train_recalls_expanded[r] = train_recalls[r]\n",
    "    train_precisions_expanded[r] = train_precisions[r]\n",
    "    train_f_scores_expanded[r] = train_f_scores[r]\n",
    "    \n",
    "    valid_recalls_expanded[r] = valid_recalls[r]\n",
    "    valid_precisions_expanded[r] = valid_precisions[r]\n",
    "    valid_f_scores_expanded[r] = valid_f_scores[r]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record experiment configuration and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_scoring.record_model_creation(\n",
    "    model_id, notes, place_images, label_suffix+label_lot, resolution, stack_label, feature_count, \n",
    "    window, generator_t.remapping, balancing, \n",
    "    network.get_config(), epochs_fast+epochs_slow, batch_size,\n",
    "    train_confusion, train_recalls_expanded, train_precisions_expanded, train_accuracy,\n",
    "    train_f_scores, train_f_score_average,\n",
    "    valid_confusion, valid_recalls_expanded, valid_precisions_expanded, valid_accuracy,\n",
    "    valid_f_scores, valid_f_score_average, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shutdown_system:\n",
    "    print('\\n'*4)\n",
    "    print(\"========================\")\n",
    "    print(\"========================\")\n",
    "    print(\"==== sudo poweroff =====\")\n",
    "    print(\"========================\")\n",
    "    print(\"========================\")\n",
    "    print('\\n'*4)\n",
    "    print(\"!dev-goodbye!\")\n",
    "\n",
    "    os.system('sudo poweroff')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
