{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development: Score composite map/product\n",
    "Generally scoring has been a matter of applying the model to samples with known LULC, and then comparing predicted to actual LULC. This approach does not work for directly scoring maps, such as a composite/mode map, because they do not represent a direct model output. This notebook demonstrates scoring an actual map, based on a DL product.\n",
    "\n",
    "Generally, this notebook assumes that the target product is a 6-category areal map, which can also \n",
    "  \n",
    "Date: 2019-02-13  \n",
    "Author: Peter Kerins  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements\n",
    "(may be over-inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "import shapely\n",
    "import cartopy\n",
    "import numpy as np\n",
    "\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import descarteslabs as dl\n",
    "print (dl.places.find('illinois')) ## TEST\n",
    "\n",
    "ULU_REPO = os.environ[\"ULU_REPO\"]\n",
    "if not ULU_REPO in sys.path:\n",
    "    sys.path.append(ULU_REPO+'/utils')\n",
    "    sys.path.append(ULU_REPO)\n",
    "print (sys.path)\n",
    "\n",
    "import util_vectors\n",
    "import util_rasters\n",
    "import util_scoring\n",
    "import util_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose city\n",
    "place = 'hyderabad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product='wri:ulu-india'\n",
    "product_year='2019'\n",
    "model_id = product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "data_root='/data/phase_iv/'\n",
    "data_path = data_root+place+'/'\n",
    "\n",
    "resolution = 5  # Lx:15 S2:10\n",
    "\n",
    "# tiling\n",
    "tile_resolution = resolution\n",
    "tile_size = 256\n",
    "tile_pad = 32\n",
    "tile_side = tile_size+tile_pad+tile_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set features of original model used for mapping\n",
    "# not used in scoring, just for record-keeping\n",
    "s2_bands=['blue','green','red','nir','swir1','swir2','alpha']; suffix='BGRNS1S2A'  # S2, Lx\n",
    "look_window=17\n",
    "\n",
    "# # ground truth source: aue, aue+osm, aue+osm2\n",
    "gt_type = 'aue'\n",
    "gt_lot = '0'\n",
    "\n",
    "stack_label, feature_count = util_workflow.build_stack_label(\n",
    "        bands_vir=s2_bands[:-1],\n",
    "        bands_sar=None,\n",
    "        bands_ndvi=None,\n",
    "        bands_ndbi=None,\n",
    "        bands_osm=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product-specific stuff\n",
    "if product_year=='2019':\n",
    "    start_datetime='2018-01-01'\n",
    "    end_datetime='2021-01-01'\n",
    "elif product_year=='2016':\n",
    "    start_datetime='2015-01-01'\n",
    "    end_datetime='2018-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [0,1,2,3,4,5]\n",
    "categories_map = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n",
    "categories_remapping = [0,1,2]\n",
    "categories_remapping_map = {0: 0, 1: 1, 2: 2, 3: 2, 4: 2, 5: 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and inspect study area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "place_title = place.title()\n",
    "place_shapefile = data_path+place_title+\"_studyAreaEPSG4326.shp\"\n",
    "\n",
    "shape = util_vectors.load_shape(place_shapefile)\n",
    "polygon = shape['geometry']['coordinates']\n",
    "place_bbox = shape['bbox']\n",
    "\n",
    "# using Albers projection\n",
    "lonlat_crs = cartopy.crs.PlateCarree()\n",
    "clat, clon = (place_bbox[0]+place_bbox[2])/2.0, (place_bbox[1]+place_bbox[3])/2.0\n",
    "print (\"center co-ordinates\", clat, clon)\n",
    "albers = cartopy.crs.AlbersEqualArea(central_latitude=clat, central_longitude=clon)\n",
    "\n",
    "# visualize Study Region\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = plt.subplot(projection=albers) # Specify projection of the map here\n",
    "shp = shapely.geometry.shape(shape['geometry'])\n",
    "ax.add_geometries([shp], lonlat_crs)\n",
    "ax.set_extent((place_bbox[0], place_bbox[2], place_bbox[1], place_bbox[3]), crs=lonlat_crs)\n",
    "ax.gridlines(crs=lonlat_crs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tiles = dl.raster.dltiles_from_shape(tile_resolution, tile_size, tile_pad, shape)\n",
    "single_tile_id = 22\n",
    "highlights = {single_tile_id:'green'}\n",
    "util_vectors.draw_tiled_area(shape, tiles, albers, lonlat_crs, highlights=highlights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare map and ground-truth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create matching lists (ie the nth element in one list refers to the same pixel as the nth element in the other) of actual and predicted LULC, based on the rasterized ground-truth and the composite/mode map.  \n",
    "\n",
    "Simultaneously score using a full typology and the \"standard\" 3-category reduced typology. Obviously this is only applicable in the case of an areal (ie not roads) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat_full = []\n",
    "Y_full = []\n",
    "\n",
    "Yhat_remapping = []\n",
    "Y_remapping = []\n",
    "\n",
    "# load up locale lists if appropriate\n",
    "if v_only:\n",
    "    place_locales_filename = data_root+'models/'+'locales_'+place+'.pkl'\n",
    "    with open(place_locales_filename, \"rb\") as f:\n",
    "        place_locales = pickle.load(f,encoding='latin1')\n",
    "    t_locales, v_locales = place_locales[place][0], place_locales[place][1]\n",
    "    print(type(v_locales), v_locales)\n",
    "\n",
    "for tile_id in range(len(tiles['features'])):\n",
    "    if tile_id % 100 == 0:\n",
    "        print('progress at tile #',tile_id)\n",
    "    # get ground-truth tile\n",
    "    gt_tile_path = (data_path+'gt'+'/'+place+'_'+gt_type+gt_lot+'_'+ \n",
    "                    str(tile_resolution)+'m'+'_'+'p'+str(tile_pad)+'_'+\n",
    "                    'tile'+str(tile_id).zfill(4)+'_'+'lulc'+'.tif')\n",
    "    tile_gt, geo, prj, cols, rows = util_rasters.load_geotiff(gt_tile_path,dtype='uint8')\n",
    "    if tile_gt.shape != (tile_side, tile_side):\n",
    "        raise Exception(\"bad gt tile shape:\",tile_gt.shape)\n",
    "    \n",
    "    Y_img = tile_gt[tile_pad:-tile_pad,tile_pad:-tile_pad]\n",
    "    # count non-no-data pixels (255), excluding padding, and skip those without gt\n",
    "    n_pixels = np.sum(Y_img!=255) \n",
    "    if n_pixels == 0:\n",
    "        continue\n",
    "    \n",
    "    print('processing non-empty gt tile: #',tile_id)\n",
    "    print('categories present: ',np.unique(Y_img))\n",
    "    \n",
    "    # load locale tile if appropriate\n",
    "    if v_only:\n",
    "        locale_tile_path = (data_path+'gt'+'/'+place+'_'+gt_type+gt_lot+'_'+ \n",
    "                        str(tile_resolution)+'m'+'_'+'p'+str(tile_pad)+'_'+\n",
    "                        'tile'+str(tile_id).zfill(4)+'_'+'locale'+'.tif')\n",
    "        tile_locale, _,_,_,_ = util_rasters.load_geotiff(locale_tile_path,dtype='uint8')\n",
    "        if tile_locale.shape != (tile_side, tile_side):\n",
    "            raise Exception(\"bad locale tile shape:\",tile_locale.shape)\n",
    "        tile_locale_unpadded = tile_locale[tile_pad:-tile_pad,tile_pad:-tile_pad]\n",
    "        \n",
    "        locale_mask = np.isin(tile_locale_unpadded, v_locales)\n",
    "        if np.sum(locale_mask) == 0:\n",
    "            # if no pixels in a validation locale, skip to next tile\n",
    "            print('skip training-only gt tile: #',tile_id)\n",
    "            continue\n",
    "            \n",
    "        # blank out gt values for non-validation locales\n",
    "        print (np.sum(Y_img!=255), Y_img)\n",
    "        Y_img[~locale_mask] = 255\n",
    "        print (np.sum(Y_img!=255), Y_img)\n",
    "    \n",
    "    tile = tiles['features'][tile_id]\n",
    "    aoi=dl.scenes.DLTile.from_key(tile['properties']['key'])\n",
    "#     print(tile)\n",
    "    scenes, ctx = dl.scenes.search(aoi,products=[product],start_datetime=start_datetime,end_datetime=end_datetime)\n",
    "    \n",
    "#     print(type(ctx))\n",
    "#     print(ctx)\n",
    "#     ctx._resolution=tile_resolution\n",
    "#     print(ctx, type(ctx))\n",
    "    \n",
    "#     print(scenes)\n",
    "#     for scene in scenes:\n",
    "#         print(scene)\n",
    "    try:\n",
    "        tile_lulc = scenes.mosaic(['lulc'], ctx, \n",
    "               mask_nodata=True, mask_alpha=None, bands_axis=0, resampler='near', \n",
    "               processing_level=None, scaling=None, data_type='Byte', raster_info=False)\n",
    "    except ValueError as e: \n",
    "        print('skipping tile with empty SceneCollection!')\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    if tile_lulc.shape != (1,tile_side, tile_side):\n",
    "        raise Exception(\"bad lulc tile shape:\",tile_lulc.shape)\n",
    "    \n",
    "    Yhat_img = tile_lulc[0,tile_pad:-tile_pad,tile_pad:-tile_pad]\n",
    "    \n",
    "    Yhat, Y = util_scoring.extract_scoring_arrays(Yhat_img, Y_img, categories, remapping=None)\n",
    "    Yhat_alt, Y_alt = util_scoring.extract_scoring_arrays(Yhat_img, Y_img, categories_remapping, remapping='standard')\n",
    "#     print(Yhat[0:20])\n",
    "#     print(Yhat_alt[0:20])\n",
    "\n",
    "    # this is where we test for an empty product\n",
    "    # ie where map of urban india excluded portion of aue study area\n",
    "    if (np.ma.is_masked(Yhat)) and len(np.unique(Yhat))==1:\n",
    "        print ('skipping unmapped tile!')\n",
    "        print (tile)\n",
    "        continue\n",
    "        \n",
    "#     print(Yhat.shape, Y.shape)\n",
    "    confusion = util_scoring.calc_confusion(Yhat, Y, categories)\n",
    "    Yhat_full.extend(Yhat)\n",
    "    Y_full.extend(Y)\n",
    "    \n",
    "    confusion = util_scoring.calc_confusion(Yhat_alt, Y_alt, categories_remapping)\n",
    "    Yhat_remapping.extend(Yhat_alt)\n",
    "    Y_remapping.extend(Y_alt)\n",
    "    \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate confusion matrix and statistics for entire city using the full, 6-category results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = util_scoring.calc_confusion(np.asarray(Yhat_full), np.asarray(Y_full), categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls, precisions, accuracy = util_scoring.calc_confusion_details(confusion)\n",
    "print (recalls)\n",
    "print (precisions)\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate f-score\n",
    "beta = 2\n",
    "f_scores = (beta**2 + 1) * precisions * recalls / ( (beta**2 * precisions) + recalls )\n",
    "f_score_average = np.mean(f_scores)\n",
    "print (f_scores)\n",
    "print (f_score_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regenerate confusion matrix and statistics for entire city using the reduced, 3-category typology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_remapping = util_scoring.calc_confusion(np.asarray(Yhat_remapping), np.asarray(Y_remapping), categories_remapping) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls_remapping, precisions_remapping, accuracy_remapping = util_scoring.calc_confusion_details(confusion_remapping)\n",
    "print (recalls_remapping)\n",
    "print (precisions_remapping)\n",
    "print (accuracy_remapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate f-score\n",
    "beta = 2\n",
    "f_scores_remapping = (beta**2 + 1) * precisions_remapping * recalls_remapping / ( (beta**2 * precisions_remapping) + recalls_remapping )\n",
    "f_score_average_remapping = np.mean(f_scores_remapping)\n",
    "print (f_scores_remapping)\n",
    "print (f_score_average_remapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log results for *full* typology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if v_only:\n",
    "    notes = 'validation ' + product_year + ' full typology '+'composite map scoring of '+place\n",
    "else:\n",
    "    notes = product_year + ' full typology '+'composite map scoring of '+place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanding lists to match expected model_record stuff\n",
    "recalls_expanded = [None,None,None,None,None,None,None,]\n",
    "precisions_expanded = [None,None,None,None,None,None,None,]\n",
    "f_scores_expanded = [None,None,None,None,None,None,None,]\n",
    "for r in range(0,len(recalls)):\n",
    "    recalls_expanded[r] = recalls[r]\n",
    "    precisions_expanded[r] = precisions[r]\n",
    "    f_scores_expanded[r] = f_scores[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_scoring.record_model_application(\n",
    "        model_id, notes, place+'('+product_year+')', gt_type + gt_lot, resolution, stack_label, feature_count, \n",
    "        look_window, categories_map, \n",
    "        confusion, recalls_expanded, precisions_expanded, accuracy,\n",
    "        f_scores_expanded, f_score_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log results for *reduced* typology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if v_only:\n",
    "    notes_remapping = 'validation ' + product_year + ' reduced typology '+'composite map scoring of '+place\n",
    "else:\n",
    "    notes_remapping = product_year + ' reduced typology '+'composite map scoring of '+place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanding lists to match expected model_record stuff\n",
    "recalls_remapping_expanded = [None,None,None,None,None,None,None,]\n",
    "precisions_remapping_expanded = [None,None,None,None,None,None,None,]\n",
    "f_scores_remapping_expanded = [None,None,None,None,None,None,None,]\n",
    "for r in range(0,len(recalls_remapping)):\n",
    "    recalls_remapping_expanded[r] = recalls_remapping[r]\n",
    "    precisions_remapping_expanded[r] = precisions_remapping[r]\n",
    "    f_scores_remapping_expanded[r] = f_scores_remapping[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_scoring.record_model_application(\n",
    "        model_id, notes_remapping, place+'('+product_year+')', gt_type + gt_lot, resolution, stack_label, feature_count, \n",
    "        look_window, categories_remapping_map, \n",
    "        confusion_remapping, recalls_remapping_expanded, precisions_remapping_expanded, accuracy_remapping,\n",
    "        f_scores_remapping_expanded, f_score_average_remapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
