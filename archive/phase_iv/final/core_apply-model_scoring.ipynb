{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core: Applying Classifier for Scoring\n",
    "Utilize already-trained model to classify input data, within the new chips & catalog paradigm. Notebook can theoretically be used for any type of model (eg 3-category, 6-category, etc) but any custom category consolidations would require additional code.  \n",
    "\n",
    "There are two main modes of application for a trained model:  \n",
    "(1) Apply to arbitrary set of catalog data to generate scores;  \n",
    "(2) Apply to imagery tiles in order to generate comprehensive LULC maps  \n",
    "\n",
    "This notebook deals with (1).\n",
    "\n",
    "Importantly, this notebook permits the sequential application of a model to an arbitrary number of \"images\" from any number of cities. (Here \"images\" actually refers to the chips drawn from training images.) This also permits the scoring of a single image.\n",
    "  \n",
    "Date: 2019-01-14  \n",
    "Author: Peter Kerins  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements\n",
    "(may be over-inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical, comprehensive imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import itertools\n",
    "import pickle\n",
    "import collections\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import ogr, gdal\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "\n",
    "import descarteslabs as dl\n",
    "\n",
    "ULU_REPO = os.environ[\"ULU_REPO\"]\n",
    "if not ULU_REPO in sys.path:\n",
    "    sys.path.append(ULU_REPO+'/utils')\n",
    "    sys.path.append(ULU_REPO)\n",
    "print(sys.path)\n",
    "\n",
    "import util_chips\n",
    "import util_training\n",
    "import util_scoring\n",
    "import util_workflow\n",
    "from catalog_generator import CatalogGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "data_root='/data/phase_iv/'\n",
    "\n",
    "resolution = 5  # Lx:15 S2:10\n",
    "\n",
    "# tiling\n",
    "tile_resolution = resolution\n",
    "tile_size = 256\n",
    "tile_pad = 32\n",
    "\n",
    "# misc\n",
    "s2_bands=['blue','green','red','nir','swir1','swir2','alpha']; suffix='BGRNS1S2A'  # S2, Lx\n",
    "\n",
    "# ground truth source: aue, aue+osm, aue+osm2\n",
    "label_suffix = 'aue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_suffix = 'aue'\n",
    "label_lot = '0'\n",
    "source = 's2'\n",
    "resolution = int(tile_resolution)\n",
    "resampling = 'bilinear'\n",
    "processing_level = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_cats = 3\n",
    "# remapping = '3cat'\n",
    "# categories = [0,1,2,]\n",
    "\n",
    "# category_label = {0:'Open Space',1:'Non-Residential',\\\n",
    "#                    2:'Residential Atomistic',3:'Residential Informal Subdivision',\\\n",
    "#                    4:'Residential Formal Subdivision',5:'Residential Housing Project',\\\n",
    "#                    6:'Roads',7:'Study Area',8:'Labeled Study Area',254:'No Data',255:'No Label'}\n",
    "\n",
    "# cats_map = {}\n",
    "# cats_map[0] = 0\n",
    "# cats_map[1] = 1\n",
    "# cats_map[2] = 2\n",
    "# cats_map[3] = 2\n",
    "# cats_map[4] = 2\n",
    "# cats_map[5] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cats = 6\n",
    "remapping = None\n",
    "categories = [0,1,2,3,4,5]\n",
    "\n",
    "category_label = {0:'Open Space',1:'Non-Residential',\\\n",
    "                   2:'Residential Atomistic',3:'Residential Informal Subdivision',\\\n",
    "                   4:'Residential Formal Subdivision',5:'Residential Housing Project',\\\n",
    "                   6:'Roads',7:'Study Area',8:'Labeled Study Area',254:'No Data',255:'No Label'}\n",
    "\n",
    "cats_map = {}\n",
    "cats_map[0] = 0\n",
    "cats_map[1] = 1\n",
    "cats_map[2] = 2\n",
    "cats_map[3] = 3\n",
    "cats_map[4] = 4\n",
    "cats_map[5] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_locales_bad = True\n",
    "exclude_roads = True\n",
    "exclude_locales_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set input stack and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window = 17\n",
    "\n",
    "# bands stuff outdated! needs to be reconciled with catalog filtering\n",
    "# will ignore for the moment since this is a bigger fix...\n",
    "# haven't done any examples yet incorporating additional chips beyond s2\n",
    "# into construction of a training sample\n",
    "bands_vir=s2_bands[:-1]\n",
    "bands_sar=None\n",
    "bands_ndvi=None\n",
    "bands_ndbi=None\n",
    "bands_osm=None\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Name of the model to use for prediciton\n",
    "\n",
    "# model_id = '3cat_s-asia_all_exc' # South Asia Model\n",
    "# model_id = '3cat_s+c-asia_all_exc'  # South & Central Asia Model\n",
    "# model_id = '3cat_14ct_green_2017_2-img-bl'  # 2 img, all locales\n",
    "# model_id = '3cat_14ct_green_2017_man_exLocales'  # 2 img, locales excluded \n",
    "model_id = '6cat_7city_EastAfrica' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_label, feature_count = util_workflow.build_stack_label(\n",
    "        bands_vir=bands_vir,\n",
    "        bands_sar=bands_sar,\n",
    "        bands_ndvi=bands_ndvi,\n",
    "        bands_ndbi=bands_ndbi,\n",
    "        bands_osm=bands_osm,)\n",
    "print(stack_label, feature_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model: score results\n",
    "Apply model to some set of chips and compare its predictions to the actual LULC values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select places and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_images = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_images['hindupur']=['U', 'V', 'W', 'X', 'Y', 'Z'],[13]\n",
    "place_images['singrauli']=['O','P','Q','R','S','T','U'],[38]\n",
    "place_images['vijayawada']=['H','I'],[68]\n",
    "place_images['jaipur']=['T','U','W','X','Y','Z'],[27, 72]\n",
    "place_images['hyderabad']=['P','Q','R','S','T','U'],[10, 44, 46, 60, 79, 55, 60]\n",
    "place_images['sitapur']=['Q','R','T','U','V'],[2, 27, 43]\n",
    "place_images['kanpur']=['AH', 'AK', 'AL', 'AM', 'AN'],[6, 19, 57, 67]\n",
    "place_images['belgaum']=['P','Q','R','S','T'],[13]\n",
    "place_images['parbhani']=['T','V','W','X','Y','Z'],[10, 42, 54]\n",
    "place_images['pune']=['P', 'Q', 'T', 'U', 'S'],[9, 54, 73]\n",
    "place_images['ahmedabad']= ['Z', 'V', 'W', 'X', 'Y', 'AA'],[22, 25, 45, 65, 70]\n",
    "place_images['malegaon']=  ['V', 'W', 'X', 'Y', 'Z'],[6]\n",
    "place_images['kolkata'] =  ['M','N','O','P','Q','R'],[16, 90, 105,  195, 218]\n",
    "place_images['mumbai']=['P','Q','R','S','U','V'],[24, 42, 73, 98, 99, 103, 123, 131, 133, 152, 160, 172]\n",
    "place_images['coimbatore']=['Q','R','S'],[15, 21, 68, 74]\n",
    "place_images['jalna']=['AV','AW','AX'],[12, 20, 31, 34, 42, 44, 65, 69, 73]\n",
    "place_images['kozhikode']=['J','K','L'],[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place_images['dhaka']=['A','B','C'],[72]\n",
    "# place_images['saidpur']=['A','B','C'],[2, 21, 32, 39, 43, 47, 52]\n",
    "# place_images['rajshahi']=['A','B','C'],[17]\n",
    "# place_images['lahore']=['A','B','C'],[33, 70]\n",
    "# place_images['karachi']=['A','B','C'],[20, 29, 62]\n",
    "# place_images['sialkot']=['A','B','C'],[32, 53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place_images['gorgan']=['A','B','C'],[36, 59, 69]\n",
    "# place_images['qom']=['A','B','C'],[1]\n",
    "# place_images['tehran']=['A','B','C'],[28, 56, 76]\n",
    "# place_images['shymkent']=['A','B','C'],[62]\n",
    "# place_images['pokhara']=['A','B','C'],[25, 28, 31, 49, 51]\n",
    "# place_images['bukhara']=['A','B','C'],[61]\n",
    "# place_images['tashkent']=['A','B','C'],[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place_images['culiacan']=['A', 'B'],[1, 9, 29, 51, 61, 66, 71]\n",
    "# place_images['guadalajara']=['A', 'B'],[22, 47, 65]\n",
    "# place_images['leon']=['A', 'B'],[3, 16, 27, 36, 38, 46, 67]\n",
    "# place_images['mexico-city']=['A', 'B'],[0, 13, 57, 75, 112, 183, 198]\n",
    "# place_images['reynosa']=['A', 'B'],[25, 31, 40, 52, 61]\n",
    "# place_images['tijuana']=['A', 'B'],[9, 45, 49, 53]\n",
    "# place_images['merida']=['A', 'B'],[25, 55, 57]\n",
    "# place_images['monterrey']=['A', 'B'],[1]\n",
    "# place_images['tuxtla']=['A', 'B'],[3, 20, 58, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "category_weights_filename =data_root+'models/'+model_id+'_category_weights.pkl'\n",
    "category_weights = pickle.load( open( category_weights_filename, \"rb\" ) )\n",
    "weights = list(zip(*category_weights.items()))[1]\n",
    "print(weights)\n",
    "\n",
    "# Load the model\n",
    "network_filename = data_root+'models/'+model_id+'.hd5'\n",
    "network = load_model(network_filename, custom_objects={'loss': util_training.make_loss_function_wcc(weights)})\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = util_chips.load_catalog()\n",
    "print(len(df.index))\n",
    "\n",
    "new_places = [\n",
    "    'dhaka',\n",
    "    'saidpur',\n",
    "    'rajshahi',\n",
    "    'lahore',\n",
    "    'karachi',\n",
    "    'sialkot',\n",
    "    'coimbatore',\n",
    "    'jalna',\n",
    "    'kozhikode',\n",
    "    'bukhara',\n",
    "    'gorgan',\n",
    "    'pokhara',\n",
    "    'qom',\n",
    "    'shymkent',\n",
    "    'tashkent',\n",
    "    'tehran',\n",
    "    'culiacan',\n",
    "    'guadalajara',\n",
    "    'leon',\n",
    "    'reynosa',\n",
    "    'tijuana',\n",
    "    'merida',\n",
    "    'monterrey',\n",
    "    'tuxtla',\n",
    "]\n",
    "\n",
    "included_places = list(set(new_places) & set(place_images.keys()))\n",
    "\n",
    "for place in included_places:\n",
    "    print(place)\n",
    "    place_catalog_path = data_root+'chip_catalog_'+place+'.csv'\n",
    "    print(place_catalog_path)\n",
    "\n",
    "    df_place = pd.read_csv(place_catalog_path)\n",
    "    print('no of chips:', len(df_place))\n",
    "\n",
    "    df = df.append(df_place, ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print(len(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training/validation locale assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_place_locales = {}\n",
    "for place in place_images:\n",
    "    place_locales_filename = data_root+'models/'+'locales_'+place+'.pkl'\n",
    "    with open(place_locales_filename, \"rb\") as f:\n",
    "        place_locales = pickle.load(f,encoding='latin1')\n",
    "#         print(place_locales)\n",
    "    combined_place_locales.update(place_locales)\n",
    "pprint(combined_place_locales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequentially apply model to every included \"image\" (ie set of chips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through each imagery from above and score \n",
    "for place,image_list in place_images.items():    \n",
    "    \n",
    "#     for image in ### Select places and imagesimage_list:  #includes all locales\n",
    "    for image in image_list[0]:  #filters bad locales\n",
    "\n",
    "        notes = 'application of ' + model_id + ' to 2017 green imagery from ' + place + '(' + image + ')'\n",
    "        print(notes)\n",
    "        \n",
    "        mask = pd.Series(data=np.zeros(len(df.index),dtype='uint8'), index=range(len(df)), dtype='uint8')\n",
    "        \n",
    "        mask |= (df['city']==place) & (df['image']==image)\n",
    "        \n",
    "        if exclude_locales_training:\n",
    "            mask &= (df['locale'].isin(combined_place_locales[place][1]))\n",
    "        \n",
    "        if exclude_roads:\n",
    "            mask &= (df['lulc']!=6)\n",
    "\n",
    "        mask &= (df['gt_type']==label_suffix)\n",
    "        mask &= (df['gt_lot']==int(label_lot))\n",
    "        mask &= (df['source']==source)\n",
    "        mask &= (df['resolution']==int(resolution))\n",
    "        mask &= (df['resampling']==resampling)\n",
    "        mask &= (df['processing']==str(processing_level).lower())\n",
    "        \n",
    "        if exclude_locales_bad:\n",
    "            mask &= (~df['locale'].isin(place_images[place][1]))\n",
    "\n",
    "        print(np.sum(mask))\n",
    "        \n",
    "        df_sub = df[mask]\n",
    "        df_sub.reset_index(drop=True,inplace=True)\n",
    "        print(len(df_sub))\n",
    "        \n",
    "        generator = CatalogGenerator(df_sub,remapping=remapping,look_window=window,batch_size=batch_size,one_hot=n_cats)\n",
    "        \n",
    "        generator.reset()\n",
    "        #predict_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
    "        predictions = network.predict_generator(generator, steps=generator.steps, verbose=1,\n",
    "                          use_multiprocessing=False, max_queue_size=40, workers=64,)\n",
    "        print(predictions.shape)\n",
    "        \n",
    "        Yhat = predictions.argmax(axis=-1)\n",
    "        print(Yhat.shape)\n",
    "        \n",
    "        Y = generator.get_label_series().values\n",
    "        print(Y.shape)\n",
    "        \n",
    "        print(\"evaluate validation\")\n",
    "        \n",
    "        confusion = util_scoring.calc_confusion(Yhat,Y,categories)\n",
    "        recalls, precisions, accuracy = util_scoring.calc_confusion_details(confusion)\n",
    "\n",
    "        # Calculate f-score\n",
    "        beta = 2\n",
    "        f_scores = (beta**2 + 1) * precisions * recalls / ( (beta**2 * precisions) + recalls )\n",
    "        f_score_average = np.mean(f_scores)\n",
    "        \n",
    "        # expanding lists to match expected model_record stuff\n",
    "        recalls_expanded = [None,None,None,None,None,None,None,]\n",
    "        precisions_expanded = [None,None,None,None,None,None,None,]\n",
    "        f_scores_expanded = [None,None,None,None,None,None,None,]\n",
    "        for r in range(0,len(recalls)):\n",
    "            recalls_expanded[r] = recalls[r]\n",
    "            precisions_expanded[r] = precisions[r]\n",
    "            f_scores_expanded[r] = f_scores[r]\n",
    "        util_scoring.record_model_application(\n",
    "                model_id, notes, place + '(' + image + ')', label_suffix, resolution, stack_label, feature_count, \n",
    "                generator.look_window, cats_map, \n",
    "                confusion, recalls_expanded, precisions_expanded, accuracy,\n",
    "                f_scores_expanded, f_score_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
