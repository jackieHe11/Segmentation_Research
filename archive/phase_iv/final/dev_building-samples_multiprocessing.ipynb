{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development: Building Standalone Training Samples\n",
    "Within new architecture, training samples will be built on the fly from standalone image \"chips\". These chips, however, will be precreated, static files that are stored separately from the source imagery and ground-truth from which they are derived. Simultaneously, the relevant features of every single chip will be stored in a catalog (csv/dataframe).\n",
    "\n",
    "Author: Eric, Taufiq  \n",
    "Date: Summer 2019  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! which gdal_translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import os, sys\n",
    "import shapely\n",
    "import cartopy\n",
    "import numpy as np\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import descarteslabs as dl\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import gdal\n",
    "\n",
    "ULU_REPO = os.environ[\"ULU_REPO\"]\n",
    "sys.path.append(ULU_REPO+'/utils')\n",
    "sys.path.append(ULU_REPO)\n",
    "print(sys.path)\n",
    "import util_vectors\n",
    "import util_rasters\n",
    "import util_chips_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_old = pd.read_csv('/data/phase_iv/chip_catalog.csv')\n",
    "# df_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_resolution = 5\n",
    "tile_size = 256\n",
    "tile_pad = 32\n",
    "\n",
    "processing_level = None\n",
    "source = 's2'\n",
    "\n",
    "s2_bands=['blue','green','red','nir','swir1','swir2','alpha']; suffix='BGRNS1S2A'  # S2, Lx\n",
    "resolution=tile_resolution  # Lx:15 S2:10\n",
    "\n",
    "#s1_bands=['vv','vh']; s1_suffix='VVVH'  \n",
    "\n",
    "resampling='bilinear'\n",
    "\n",
    "label_suffix = 'aue'\n",
    "label_lot = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select city\n",
    "# loop through ground truth tiles\n",
    "# for each tile:\n",
    "# get locations of target pixels\n",
    "# loop through list of locations of target pixels\n",
    "# for each pixel:\n",
    "# grab a window of imagery centered around target pixel\n",
    "# save this as a separate \"image chip\" geotiff, with careful naming\n",
    "# as each chip is saved, add an entry to a catalog dataframe\n",
    "# save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from multiprocessing import Process, cpu_count\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from datetime import datetime\n",
    "import gdal\n",
    "\n",
    "\n",
    "#\n",
    "# CONFIG\n",
    "#\n",
    "MAX_POOL_PROCESSES=cpu_count()-1\n",
    "MAX_THREADPOOL_PROCESSES=16\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# METHODS\n",
    "#\n",
    "\"\"\" MAP METHODS\n",
    "  Args:\n",
    "    * map_function <function>: \n",
    "      a function to map over args list. the function should take a single argument.\n",
    "      if multiple arguments are needed accept them as a single list or tuple\n",
    "    * args_list <list>: the list of arguments to map over\n",
    "    * max_process <int>: number of processes\n",
    "      - for max_with_pool defaults to the number of cpus minus 1\n",
    "      - for max_with_threadpool defaults to 16\n",
    "      - map_sequential ignores this argument as its doesn't actually do \n",
    "        any multiprocesssing \n",
    "  Return:\n",
    "    List of return values from map_function\n",
    "  Notes:\n",
    "    map_sequential does NOT multiprocess.  it can be used as a sequential drop-in \n",
    "    replacement for map_with_pool/threadpool.  this is useful for:\n",
    "      - development \n",
    "      - debugging\n",
    "      - benchmarking \n",
    "\"\"\"\n",
    "def map_with_pool(map_function,args_list,max_processes=MAX_POOL_PROCESSES):\n",
    "  pool=Pool(processes=min(len(args_list),max_processes))\n",
    "  return _run_pool(pool,map_function,args_list)\n",
    "\n",
    "\n",
    "def map_with_threadpool(map_function,args_list,max_processes=MAX_THREADPOOL_PROCESSES):\n",
    "  pool=ThreadPool(processes=min(len(args_list),max_processes))\n",
    "  return _run_pool(pool,map_function,args_list)\n",
    "\n",
    "\n",
    "def map_sequential(map_function,args_list,print_args=False,noisy=False,**dummy_kwargs):\n",
    "  if noisy:\n",
    "    print('multiprocessing(test):')\n",
    "  out=[]\n",
    "  for i,args in enumerate(args_list):\n",
    "      if noisy: \n",
    "        print('\\t{}...'.format(i))\n",
    "      if print_args:\n",
    "        print('\\t{}'.format(args))\n",
    "      out.append(map_function(args))\n",
    "  if noisy: \n",
    "    print('-'*25)\n",
    "  return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" simple: vanilla multiprocessing\n",
    "  Args:\n",
    "    * function <function>: function. function can take multiple arguments \n",
    "    * args_list <list>: the list of argument lists\n",
    "    * join <bool[True]>: join processes before return\n",
    "  Return: \n",
    "    List of processes \n",
    "\"\"\"\n",
    "def simple(function,args_list,join=True):\n",
    "  procs=[]\n",
    "  for args in args_list:\n",
    "      proc=Process(\n",
    "          target=function, \n",
    "          args=args)\n",
    "      procs.append(proc)\n",
    "      proc.start()\n",
    "  if join:\n",
    "    for proc in procs:\n",
    "        proc.join()\n",
    "  return procs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" MPList\n",
    "Run the above methods on map_function,args_list pairs where the map_function\n",
    "changes for each new set of args in args_list\n",
    "Args:\n",
    "    pool_type<str>: \n",
    "        one of MPList.POOL|THREAD|SEQUENTIAL.  determines which map_function \n",
    "        and default max_processes to use. If not MPList.THREAD|SEQUENTIAL it \n",
    "        will default to MPList.POOL.\n",
    "    max_processes<int>:\n",
    "        if not passed will set default based on pool_type\n",
    "    jobs<list>:\n",
    "        list of (target,args,kwargs) tuples. Note: use the append method rather than\n",
    "        creating (target,args,kwargs) tuples\n",
    "        \n",
    "\"\"\"\n",
    "class MPList():\n",
    "    #\n",
    "    # POOL TYPES\n",
    "    #\n",
    "    POOL='pool'\n",
    "    THREAD='threading'\n",
    "    SEQUENTIAL='sequential'\n",
    "    \n",
    "\n",
    "    #\n",
    "    # PUBLIC\n",
    "    #\n",
    "    def __init__(self,pool_type=None,max_processes=None,jobs=None):\n",
    "        self.pool_type=pool_type or self.POOL\n",
    "        self.max_processes=max_processes\n",
    "        self.jobs=jobs or []\n",
    "\n",
    "        \n",
    "    def append(self,target,*args,**kwargs):\n",
    "        self.jobs.append((target,)+(args,)+(kwargs,))\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        self.start_time=datetime.now()\n",
    "        map_func,self.max_processes=self._map_func_max_processes()\n",
    "        out=map_func(self._target,self.jobs,max_processes=self.max_processes)\n",
    "        self.end_time=datetime.now()\n",
    "        self.duration=str(self.end_time-self.start_time)\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.jobs)\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # INTERNAL\n",
    "    #    \n",
    "    def _map_func_max_processes(self):\n",
    "        if self.pool_type==MPList.THREAD:\n",
    "            map_func=map_with_threadpool\n",
    "            max_processes=self.max_processes or MAX_THREADPOOL_PROCESSES\n",
    "        elif self.pool_type==MPList.SEQUENTIAL:\n",
    "            map_func=map_sequential\n",
    "            max_processes=False\n",
    "        else:\n",
    "            map_func=map_with_pool\n",
    "            max_processes=self.max_processes or MAX_POOL_PROCESSES\n",
    "        return map_func, max_processes\n",
    "        \n",
    "        \n",
    "    def _target(self,args):\n",
    "        target,args,kwargs=args\n",
    "        return target(*args,**kwargs)\n",
    "        \n",
    "    \n",
    "\n",
    "#\n",
    "# INTERNAL METHODS\n",
    "#\n",
    "def _stop_pool(pool,success=True):\n",
    "  pool.close()\n",
    "  pool.join()\n",
    "  return success\n",
    "\n",
    "\n",
    "def _map_async(pool,map_func,objects):\n",
    "  try:\n",
    "    return pool.map_async(map_func,objects)\n",
    "  except KeyboardInterrupt:\n",
    "    print(\"Caught KeyboardInterrupt, terminating workers\")\n",
    "    pool.terminate()\n",
    "    return False\n",
    "  else:\n",
    "    print(\"Failure\")\n",
    "    return _stop_pool(pool,False)\n",
    "\n",
    "\n",
    "def _run_pool(pool,map_function,args_list):\n",
    "  out=_map_async(pool,map_function,args_list)\n",
    "  _stop_pool(pool)\n",
    "  return out.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_dict_decorator(func):\n",
    "    def decorator(arg_dict):\n",
    "        return func(**arg_dict)\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@arg_dict_decorator\n",
    "def generate_chips(image_suffix, place, data_root='/data/phase_iv/', \n",
    "                   label_suffix= label_suffix, label_lot= label_lot,\n",
    "                   source= source,  bands=s2_bands,\n",
    "                   resampling=resampling, processing_level=processing_level,\n",
    "                   chip_radius=32,\n",
    "                   remove_duplicates=True,\n",
    "                   category_label={0:'Open Space',1:'Non-Residential',\\\n",
    "                       2:'Residential Atomistic',3:'Residential Informal Subdivision',\\\n",
    "                       4:'Residential Formal Subdivision',5:'Residential Housing Project',\\\n",
    "                       6:'Roads',7:'Study Area',8:'Labeled Study Area',254:'No Data',255:'No Label'},\n",
    "                   show_stats=False,\n",
    "                   tile_start=None,\n",
    "                   tile_stop=None\n",
    "                  ):\n",
    "    \n",
    "    place_title = place.title()\n",
    "    place_shapefile = data_root+place+'/'+place_title+\"_studyAreaEPSG4326.shp\"\n",
    "\n",
    "    shape = util_vectors.load_shape(place_shapefile)\n",
    "\n",
    "    tiles = dl.raster.dltiles_from_shape(tile_resolution, tile_size, tile_pad, shape)\n",
    "\n",
    "    resolution = int(tiles['features'][0]['properties']['resolution'])\n",
    "    size = int(tiles['features'][0]['properties']['tilesize'])\n",
    "    pad = int(tiles['features'][0]['properties']['pad'])\n",
    "\n",
    "    if resolution==10:\n",
    "        zfill=3\n",
    "    elif resolution==5:\n",
    "        zfill=4\n",
    "    elif resolution==2:\n",
    "        zfill=5 \n",
    "    else:\n",
    "        raise Exception('bad resolution: '+str(resolution))\n",
    "        \n",
    "\n",
    "    rows_list = []\n",
    "    # select city\n",
    "    # loop through ground truth tiles\n",
    "    if tile_start is None:\n",
    "        tile_start = 0;\n",
    "    if tile_stop is None:\n",
    "        tile_stop = len(tiles['features'])\n",
    "    for tile_id in range(tile_start, tile_stop):\n",
    "        # for each tile:\n",
    "            # sample file name: /data/phase_iv/sitapur/gt/sitapur_aue0_5m_p32_tile0586_lulc.tif\n",
    "        path_base = data_root+place+'/gt/'+place+'_'+label_suffix+label_lot+'_'+str(resolution)+'m'+'_'+\\\n",
    "            'p'+str(pad)+'_'+'tile'+str(tile_id).zfill(zfill)\n",
    "        #print(path_base)\n",
    "        path_lulc = path_base+'_'+'lulc.tif'\n",
    "        path_locale = path_base+'_'+'locale.tif'\n",
    "        lulc,_,_,_,_ = util_rasters.load_geotiff(path_lulc,dtype='uint8')\n",
    "        locale,_,_,_,_ = util_rasters.load_geotiff(path_locale,dtype='uint8')\n",
    "        # 'erase' irrelevant pixels\n",
    "        lulc[0:pad,:] = 255; lulc[-pad:,:] = 255; lulc[:,0:pad] = 255; lulc[:,-pad:] = 255\n",
    "        locale[0:pad,:] = 255; locale[-pad:,:] = 255; locale[:,0:pad] = 255; locale[:,-pad:] = 255\n",
    "        # get locations of target pixels\n",
    "        locs = np.where(lulc!=255)\n",
    "        n_px = len(locs[0])\n",
    "        if n_px == 0: \n",
    "            continue\n",
    "        if show_stats:\n",
    "            util_rasters.stats_byte_raster(lulc, category_label, lulc=True, show=True)\n",
    "        print(place + ' '+ image_suffix + ': valid pixels in tile'+str(tile_id).zfill(zfill)+':', len(locs[0]))\n",
    "        #image path example: /data/phase_iv/sitapur/imagery/none/sitapur_s2_E_5m_p32_tile0006.tif\n",
    "        path_image = data_root+place+'/imagery/'+str(processing_level).lower()+'/'+place+'_'+source+'_'+\\\n",
    "            image_suffix+'_'+str(resolution)+'m'+'_'+'p'+str(pad)+'_'+'tile'+str(tile_id).zfill(zfill)+'.tif'\n",
    "        # loop through list of locations of target pixels\n",
    "        for i in range(len(locs[0])):\n",
    "            row = locs[0][i]\n",
    "            col = locs[1][i]\n",
    "            #print(row,col,lulc[row,col])\n",
    "            # for each pixel:\n",
    "            # grab a window of imagery centered around target pixel\n",
    "            xoff = col - chip_radius; yoff = row - chip_radius;\n",
    "            xsize = chip_radius*2+1; ysize = chip_radius*2+1\n",
    "            # save this as a separate \"image chip\" geotiff, with careful naming\n",
    "            path_chip = data_root+place+'/chips/'+str(processing_level).lower()+'/'+\\\n",
    "                place+'_'+label_suffix+label_lot+'_'+source+'_'+image_suffix+'_'+str(resolution)+'m'+'_'+\\\n",
    "                't'+str(tile_id).zfill(zfill)+'_'+'x'+str(col-pad).zfill(3)+'y'+str(row-pad).zfill(3)+'_'+\\\n",
    "                'c'+str(lulc[row,col])+'.tif'\n",
    "            #print(path_image)\n",
    "            #print(path_chip)\n",
    "            #print(xoff, yoff, xsize, ysize)\n",
    "            #gdal template: gdal_translate -srcwin xstart ystart xstop ystop input.raster output.raster\n",
    "            #!gdal_translate -q -srcwin {xoff} {yoff} {xsize} {ysize} {path_image} {path_chip}\n",
    "            \n",
    "            gdal.Translate(path_chip,path_image,srcWin=[xoff,yoff,xsize,ysize]) #added due to gdal_translate issues\n",
    "            \n",
    "#             commented out due to gdal_translate issues \n",
    "#             command = 'gdal_translate -q -srcwin {0} {1} {2} {3} {4} {5}'.format(xoff,yoff,xsize,ysize,path_image,path_chip)\n",
    "            #print('>>>',command)\n",
    "#             try:\n",
    "#                 subprocess.check_output(command.split(), shell=False)\n",
    "#             except subprocess.CalledProcessError as e:\n",
    "#                 raise RuntimeError(\"command '{}' return with error (code {}): {}\".format(e.cmd, e.returncode, e.output))\n",
    "            \n",
    "    \n",
    "            # as each chip is saved, add an entry to a catalog dataframe\n",
    "            #['city','gt_type','gt_lot','locale','source','image','bands',\n",
    "            #   'resolution','resampling','processing','tile_id','column','row','lulc']\n",
    "            row_dict = {}\n",
    "            row_dict['path']=path_chip\n",
    "            row_dict['city']=place\n",
    "            row_dict['gt_type']=label_suffix\n",
    "            row_dict['gt_lot']=label_lot\n",
    "            row_dict['locale']=locale[row,col]\n",
    "            row_dict['source']=source\n",
    "            row_dict['image']=image_suffix\n",
    "            row_dict['bands']=bands\n",
    "            row_dict['resolution']=resolution\n",
    "            row_dict['resampling']=resampling\n",
    "            row_dict['processing']=str(processing_level).lower()\n",
    "            row_dict['tile_id']=tile_id\n",
    "            row_dict['x']=col-pad\n",
    "            row_dict['y']=row-pad\n",
    "            row_dict['lulc']=lulc[row,col]\n",
    "            rows_list.append(row_dict)\n",
    "            \n",
    "    columns = ['path','city','gt_type','gt_lot','locale','source','image','bands',\n",
    "               'resolution','resampling','processing','tile_id','x','y','lulc']\n",
    "    df_new = pd.DataFrame(rows_list, columns=columns)\n",
    "    #DataFrame.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False)\n",
    "    #df_new.set_index('path',drop=True,append=False,inplace=True,verify_integrity=True)\n",
    "    df_new.head()\n",
    "    # save dataframe\n",
    "    path_catalog = data_root+place+'_chip_catalog.csv'  # WRITTING THE CHIPS DATA FROM EACH CITIES TO UNIQUE CSV FILES\n",
    "    if not os.path.isfile(path_catalog):\n",
    "        #write new records directly\n",
    "        df_new.to_csv(path_catalog,index=False,header=True)\n",
    "    else:\n",
    "        #read csv\n",
    "        #load to dataframe\n",
    "        df_old = pd.read_csv(path_catalog)\n",
    "        # append new\n",
    "        #DataFrame.append(other, ignore_index=False, verify_integrity=False, sort=None)\n",
    "        df_combo = df_old.append(df_new,ignore_index=False,verify_integrity=False)\n",
    "        #remove duplicates\n",
    "        if remove_duplicates:\n",
    "            #DataFrame.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "            df_combo.drop_duplicates(subset='path',keep='first',inplace=True)\n",
    "        df_combo.to_csv(path_catalog,index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_list1=[    \n",
    "        {'image_suffix':'A', 'place':'johannesburg'},\n",
    "        {'image_suffix':'A', 'place':'kampala'},\n",
    "        {'image_suffix':'A', 'place':'port_elizabeth'},\n",
    "        {'image_suffix':'A', 'place':'kigali'},\n",
    "        {'image_suffix':'A', 'place':'arusha'},\n",
    "        {'image_suffix':'A', 'place':'nakuru'}\n",
    "         ]\n",
    "arg_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_list2=[    \n",
    "        {'image_suffix':'B', 'place':'johannesburg'},\n",
    "        {'image_suffix':'B', 'place':'kampala'},\n",
    "        {'image_suffix':'B', 'place':'port_elizabeth'},\n",
    "        {'image_suffix':'B', 'place':'kigali'},\n",
    "        {'image_suffix':'B', 'place':'arusha'},\n",
    "        {'image_suffix':'B', 'place':'nakuru'}\n",
    "         ]\n",
    "arg_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time map_with_threadpool(generate_chips,arg_list1,max_processes=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time map_with_threadpool(generate_chips,arg_list2,max_processes=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geoml]",
   "language": "python",
   "name": "conda-env-geoml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
