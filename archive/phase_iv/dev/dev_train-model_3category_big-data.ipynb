{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development: Train 3-Category Classifier\n",
    "Train a 3-category classifier using only the non-road training samples.\n",
    "\n",
    "Workflow sidesteps some problems with TensorFlow by simplifying the training and shifting some components—multiple epochs, callback functionality, validation, etc—to manual coding.\n",
    "\n",
    "Currently, calls for training in just two epochs, one fast and one slow. May change.\n",
    "\n",
    "Date: 2019-09-03  \n",
    "Author: Peter Kerins  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical, comprehensive imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#\n",
    "import os, sys\n",
    "import json\n",
    "import itertools, collections\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geojson\n",
    "import fiona\n",
    "import ogr, gdal\n",
    "# get_ipython().magic(u'matplotlib inline')\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "# import math\n",
    "# from tensorflow.keras import models\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# import tensorflow.keras as keras\n",
    "# import tensorflow.keras.backend as K\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# from tensorflow.keras.layers import Input, Add, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "import h5py\n",
    "\n",
    "import descarteslabs as dl\n",
    "\n",
    "ULU_REPO = os.environ[\"ULU_REPO\"]\n",
    "if ULU_REPO not in sys.path:\n",
    "    sys.path.append(ULU_REPO+'/utils')\n",
    "    sys.path.append(ULU_REPO)\n",
    "print(sys.path)\n",
    "\n",
    "import util_descartes\n",
    "#import util_ml\n",
    "import util_rasters\n",
    "import util_vectors\n",
    "import util_workflow\n",
    "import util_chips\n",
    "import util_training\n",
    "import util_network\n",
    "import util_scoring\n",
    "from catalog_generator import CatalogGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set all user-defined variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root='/data/phase_iv/'\n",
    "\n",
    "resolution=5\n",
    "# tile_resolution = resolution\n",
    "# tile_size = 256\n",
    "# tile_pad = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcatalog_name = 'india_all-data'\n",
    "\n",
    "path_train = data_root+'models/'+subcatalog_name+'_train.csv'\n",
    "path_valid = data_root+'models/'+subcatalog_name+'_valid.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdown_system = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chips variables\n",
    "Only needed if selecting samples from master catalog, rather than loading subcatalog from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_new = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_level = None\n",
    "source = 's2'\n",
    "#image_suffix = 'E'\n",
    "\n",
    "s2_bands=['blue','green','red','nir','swir1','swir2','alpha']; s2_suffix='BGRNS1S2A'  # S2, Lx\n",
    "# s1_bands=['vv','vh']; s1_suffix='VVVH'  \n",
    "\n",
    "resampling='bilinear'\n",
    "processing = None\n",
    "\n",
    "label_suffix = 'aue'\n",
    "label_lot = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_images = {}\n",
    "place_images['hindupur']=['U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "place_images['singrauli']=['O','P','Q','R','S','T','U']\n",
    "place_images['vijayawada']=['H','I']\n",
    "place_images['jaipur']=['T','U','W','X','Y','Z']\n",
    "place_images['hyderabad']=['P','Q','R','S','T','U']\n",
    "place_images['sitapur']=['Q','R','T','U','V']\n",
    "place_images['kanpur']=['AH', 'AK', 'AL', 'AM', 'AN']\n",
    "place_images['belgaum']=['P','Q','R','S','T']\n",
    "place_images['parbhani']=['T','V','W','X','Y','Z']\n",
    "place_images['pune']=['P', 'Q', 'T', 'U', 'S']\n",
    "place_images['ahmedabad']= ['Z', 'V', 'W', 'X', 'Y', 'AA']\n",
    "place_images['malegaon']=  ['V', 'W', 'X', 'Y', 'Z']\n",
    "place_images['kolkata'] =  ['M','N','O','P','Q','R']\n",
    "place_images['mumbai']=['P','Q','R','S','U','V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_label = {0:'Open Space',1:'Non-Residential',\\\n",
    "#                    2:'Residential Atomistic',3:'Residential Informal Subdivision',\\\n",
    "#                    4:'Residential Formal Subdivision',5:'Residential Housing Project',\\\n",
    "#                    6:'Roads',7:'Study Area',8:'Labeled Study Area',254:'No Data',255:'No Label'}\n",
    "\n",
    "# cats_map = {}\n",
    "# cats_map[0] = 0\n",
    "# cats_map[1] = 1\n",
    "# cats_map[2] = 2\n",
    "# cats_map[3] = 2\n",
    "# cats_map[4] = 2\n",
    "# cats_map[5] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample construction variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# bands stuff outdated! needs to be reconciled with catalog filtering\n",
    "# will ignore for the moment since this is a bigger fix...\n",
    "# haven't done any examples yet incorporating additional chips beyond s2\n",
    "# into construction of a training sample\n",
    "bands_vir=s2_bands[:-1]\n",
    "bands_sar=None\n",
    "bands_ndvi=None\n",
    "bands_ndbi=None\n",
    "bands_osm=None\n",
    "\n",
    "# this can get updated when cloudmasking is added\n",
    "# haze_removal = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be updated completely; bands stuff doesn't make sense right now\n",
    "stack_label, feature_count = util_workflow.build_stack_label(\n",
    "        bands_vir=bands_vir,\n",
    "        bands_sar=bands_sar,\n",
    "        bands_ndvi=bands_ndvi,\n",
    "        bands_ndbi=bands_ndbi,\n",
    "        bands_osm=bands_osm,)\n",
    "print(stack_label, feature_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model & training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = '3cat_all_new-workflow'\n",
    "notes = 'using all data and cleaned up notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "balancing = None\n",
    "\n",
    "epochs_fast = 1\n",
    "epochs_slow = 1\n",
    "\n",
    "max_queue_size = 32\n",
    "workers = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify training & validation samples\n",
    "Construct subcatalogs containing all target training & validation samples, __or__ load them from file, according to variable `build_new`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option A: Construct subcatalogs by filtering master catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if build_new:\n",
    "    df = util_chips.load_catalog()\n",
    "    print(len(df.index))\n",
    "\n",
    "\n",
    "    mask = pd.Series(data=np.zeros(len(df.index),dtype='uint8'), index=range(len(df)), dtype='uint8')\n",
    "\n",
    "    for place,image_list in place_images.items():\n",
    "        for image in image_list:\n",
    "            mask |= (df['city']==place) & (df['image']==image)\n",
    "\n",
    "    # straight away remove road samples\n",
    "    mask &= (df['lulc']!=6)\n",
    "\n",
    "    # filter others according to specifications\n",
    "    mask &= (df['gt_type']==label_suffix)\n",
    "    mask &= (df['gt_lot']==int(label_lot))\n",
    "    mask &= (df['source']==source)\n",
    "    mask &= (df['resolution']==int(resolution))\n",
    "    mask &= (df['resampling']==resampling)\n",
    "    mask &= (df['processing']==str(processing).lower())\n",
    "\n",
    "    print(np.sum(mask))\n",
    "\n",
    "    df = df[mask]\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    len(df)\n",
    "\n",
    "\n",
    "\n",
    "    place_locales_paths = [\n",
    "        '/data/phase_iv/models/3cat_Ahm_V-AA_place_locales.pkl',\n",
    "        '/data/phase_iv/models/3cat_Bel_P-T_place_locales.pkl'       ,\n",
    "        '/data/phase_iv/models/3cat_Hin_U-Z_place_locales.pkl'       ,\n",
    "        '/data/phase_iv/models/3cat_Hyd_P-U_place_locales.pkl'       ,\n",
    "        '/data/phase_iv/models/3cat_Jai_T-U+W-Z_place_locales.pkl'   ,\n",
    "        '/data/phase_iv/models/3cat_Kan_AH+AK-AN_place_locales.pkl'  ,\n",
    "        '/data/phase_iv/models/3cat_Mal_V-Z_place_locales.pkl'       ,\n",
    "        '/data/phase_iv/models/3cat_Par_T+V-Z_place_locales.pkl',\n",
    "        '/data/phase_iv/models/3cat_Pun_P-Q+S-U_place_locales.pkl',\n",
    "        '/data/phase_iv/models/3cat_Sin_O-U_place_locales.pkl',\n",
    "        '/data/phase_iv/models/3cat_Sit_Q-R+T-V_place_locales.pkl',\n",
    "        '/data/phase_iv/models/3cat_Vij_H-I_place_locales.pkl',\n",
    "        '/data/phase_iv/models/3cat_Kol_M-R_place_locales.pkl',\n",
    "        '/data/phase_iv/models/3cat_Mum_P-V_place_locales.pkl'\n",
    "    ]\n",
    "\n",
    "    combined_place_locales = {}\n",
    "    for place_locales_filename in place_locales_paths:\n",
    "        with open(place_locales_filename, \"rb\") as f:\n",
    "            place_locales = pickle.load(f,encoding='latin1')\n",
    "        combined_place_locales.update(place_locales)\n",
    "#     print(combined_place_locales)\n",
    "\n",
    "\n",
    "    df_t, df_v = util_chips.mask_locales(df, combined_place_locales)\n",
    "    print(len(df_t), len(df_v))\n",
    "\n",
    "    # save the datasets for future use\n",
    "    %time df_t.to_csv(path_train,index=False)\n",
    "    %time df_v.to_csv(path_valid,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option B: Load existing subcatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not build_new:\n",
    "    df_t = pd.read_csv(path_train, encoding='utf8')\n",
    "    df_v = pd.read_csv(path_valid, encoding='utf8')\n",
    "    print(len(df_t), len(df_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect selected samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train:')\n",
    "print(util_training.calc_category_counts(df_t,remapping=None), len(df_t))\n",
    "print('valid:')\n",
    "print(util_training.calc_category_counts(df_v,remapping=None), len(df_v))\n",
    "print()\n",
    "if build_new:\n",
    "    print('all:')\n",
    "    print(util_training.calc_category_counts(df,remapping=None), len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate class weighting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2=pd.concat([df_t,df_v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_weights = util_training.generate_category_weights(df_t,remapping='standard',log=False,mu=1.0,max_score=None)\n",
    "print(category_weights.items())\n",
    "weights = list(zip(*category_weights.items()))[1]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_weights_filename = data_root+'models/'+model_id+'_category_weights.pkl'\n",
    "\n",
    "if os.path.exists(category_weights_filename):\n",
    "    raise Exception('Cannot save category weights: file already exists at specified path ('+category_weights_filename+')')\n",
    "else:\n",
    "    pickle.dump(category_weights, open(category_weights_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use weights to create weighted categorical crossentropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = util_training.make_loss_function_wcc(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build convolutional neural network and prepare it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hardcoded params\n",
    "network=util_network.build_xmodel(input_shape=(17,17,6),output_nodes=3,input_conv_block=True)\n",
    "util_network.compile_network(network, loss, LR=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct \"fast\" training with high learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sample \"generators\" (Keras _sequence_ objects) to serve samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_t = CatalogGenerator(df_t,remapping='3cat',look_window=window,batch_size=batch_size,one_hot=3)\n",
    "generator_v = CatalogGenerator(df_v,remapping='3cat',look_window=window,batch_size=batch_size,one_hot=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train fast\n",
    "#history_fast = network.fit(X_train, Y_t_cat, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, Y_v_cat), shuffle=True,callbacks=callbacks)\n",
    "#docs: fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None,\n",
    "                    #class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
    "history_fast = network.fit_generator(generator_t, epochs=epochs_fast, callbacks=None, steps_per_epoch=generator_t.steps,\n",
    "                                    #validation_data=generator_v, validation_steps=generator_v.steps,\n",
    "                                    shuffle=True,use_multiprocessing=True,max_queue_size=max_queue_size,workers=workers,)\n",
    "\n",
    "# plt.plot(history_fast.history['val_acc'])\n",
    "# plt.show()\n",
    "# plt.plot(history_fast.history['val_loss'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_weights_path = data_root + 'models/' + model_id + '_weights_fast' + '.hd5'\n",
    "print(fast_weights_path)\n",
    "network.save_weights(fast_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild model and conduct \"slow\" training with lower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hardcoded params\n",
    "network=util_network.build_xmodel(input_shape=(17,17,6),output_nodes=3,input_conv_block=True)\n",
    "# load weights from fast learning\n",
    "# network.load_weights(fast_weights_path)\n",
    "\n",
    "# util_network.compile_network(network, loss, LR=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load trained weights and prepare network for additional training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_weights(fast_weights_path)\n",
    "util_network.compile_network(network, loss, LR=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_t.reset()\n",
    "generator_v.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_slow = network.fit_generator(generator_t, epochs=epochs_slow, callbacks=None, steps_per_epoch=generator_t.steps,\n",
    "                                    #validation_data=generator_v, validation_steps=generator_v.steps,\n",
    "                                    shuffle=True,use_multiprocessing=True,max_queue_size=max_queue_size,workers=workers,)\n",
    "\n",
    "# plt.plot(history_slow.history['val_acc'])\n",
    "# plt.show()\n",
    "# plt.plot(history_slow.history['val_loss'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store further trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_weights_path = data_root + 'models/' + model_id + '_weights_slow' + '.hd5'\n",
    "print(slow_weights_path)\n",
    "network.save_weights(slow_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store entire network object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_filename = data_root+'models/'+model_id+'.hd5'\n",
    "\n",
    "if os.path.exists(network_filename):\n",
    "    raise Exception('Cannot save network: file already exists at specified path ('+network_filename+')')\n",
    "else:\n",
    "    network.save(network_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model to training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_t.reset()\n",
    "#predict_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
    "predictions_t = network.predict_generator(generator_t, steps=generator_t.steps, verbose=1,\n",
    "                  use_multiprocessing=True,max_queue_size=max_queue_size,workers=workers,)\n",
    "print(predictions_t.shape)\n",
    "\n",
    "generator_v.reset()\n",
    "#predict_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
    "predictions_v = network.predict_generator(generator_v, steps=generator_v.steps, verbose=1,\n",
    "                  use_multiprocessing=True,max_queue_size=max_queue_size,workers=workers,)\n",
    "print(predictions_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat_t = predictions_t.argmax(axis=-1)\n",
    "print(Yhat_t.shape)\n",
    "Yhat_v = predictions_v.argmax(axis=-1)\n",
    "print(Yhat_v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract corresponding _actual_ ground-truth values directly from catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_t = generator_t.get_label_series().values\n",
    "print(Y_t.shape)\n",
    "Y_v = generator_v.get_label_series().values\n",
    "print(Y_v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate typical scoring information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"evaluate training\")\n",
    "# hardcoded categories\n",
    "categories=[0,1,2]\n",
    "train_confusion = util_scoring.calc_confusion(Yhat_t,Y_t,categories)\n",
    "train_recalls, train_precisions, train_accuracy = util_scoring.calc_confusion_details(train_confusion)\n",
    "\n",
    "# Calculate f-score\n",
    "beta = 2\n",
    "train_f_score = (beta**2 + 1) * train_precisions * train_recalls / ( (beta**2 * train_precisions) + train_recalls )\n",
    "train_f_score_open = train_f_score[0] \n",
    "train_f_score_nonres = train_f_score[1]  \n",
    "train_f_score_res = train_f_score[2]  \n",
    "train_f_score_roads = None#train_f_score[3]  \n",
    "train_f_score_average = np.mean(train_f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"evaluate validation\")\n",
    "valid_confusion = util_scoring.calc_confusion(Yhat_v,Y_v,categories)\n",
    "valid_recalls, valid_precisions, valid_accuracy = util_scoring.calc_confusion_details(valid_confusion)\n",
    "\n",
    "# Calculate f-score\n",
    "valid_f_score = (beta**2 + 1) * valid_precisions * valid_recalls / ( (beta**2 * valid_precisions) + valid_recalls )\n",
    "valid_f_score_open = valid_f_score[0] \n",
    "valid_f_score_nonres = valid_f_score[1] \n",
    "valid_f_score_res = valid_f_score[2] \n",
    "valid_f_score_roads = None# valid_f_score[3] \n",
    "valid_f_score_average = np.mean(valid_f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanding lists to match expected model_record stuff\n",
    "train_recalls_expanded = [train_recalls[0],train_recalls[1],train_recalls[2],None]\n",
    "valid_recalls_expanded = [valid_recalls[0],valid_recalls[1],valid_recalls[2],None]\n",
    "train_precisions_expanded = [train_precisions[0],train_precisions[1],train_precisions[2],None]\n",
    "valid_precisions_expanded = [valid_precisions[0],valid_precisions[1],valid_precisions[2],None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record experiment configuration and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_scoring.record_model_creation(\n",
    "    model_id, notes, place_images, label_suffix+label_lot, resolution, stack_label, feature_count, \n",
    "    window, generator_t.remapping, balancing, \n",
    "    network.get_config(), epochs, batch_size,\n",
    "    train_confusion, train_recalls_expanded, train_precisions_expanded, train_accuracy,\n",
    "    train_f_score_open, train_f_score_nonres, train_f_score_res, train_f_score_roads, train_f_score_average,\n",
    "    valid_confusion, valid_recalls_expanded, valid_precisions_expanded, valid_accuracy,\n",
    "    valid_f_score_open, valid_f_score_nonres, valid_f_score_res, valid_f_score_roads, valid_f_score_average,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shutdown_system:\n",
    "    print('\\n'*4)\n",
    "    print(\"========================\")\n",
    "    print(\"========================\")\n",
    "    print(\"==== sudo poweroff =====\")\n",
    "    print(\"========================\")\n",
    "    print(\"========================\")\n",
    "    print('\\n'*4)\n",
    "    print(\"!dev-goodbye!\")\n",
    "\n",
    "    os.system('sudo poweroff')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
