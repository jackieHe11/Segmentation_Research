{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development: Applying 3-Category Classifier with New Architecture\n",
    "Utilize already-trained model to create comprehensive LULC classifications from imagery, either downloaded or on-the-fly.  \n",
    "\n",
    "This notebook is an interim product for troubleshooting as we adapt the workflows to Python 3 and, more importantly, transition to executing these functions as a script, with the additional change of using imagery getting pulled in on the fly.\n",
    "\n",
    "Using chips and scoring models is not included.\n",
    "  \n",
    "Date: 2019-06-10  \n",
    "Author: DC Team  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements\n",
    "(may be over-inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical, comprehensive imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import itertools\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import math\n",
    "#\n",
    "import numpy as np\n",
    "import shapely\n",
    "import cartopy\n",
    "import geojson\n",
    "import fiona\n",
    "import gdal\n",
    "import h5py\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import ogr, gdal\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import descarteslabs as dl\n",
    "print (dl.places.find('illinois')) ## TEST\n",
    "\n",
    "ULU_REPO = os.environ[\"ULU_REPO\"]\n",
    "sys.path.append(ULU_REPO+'/utils')\n",
    "sys.path.append(ULU_REPO)\n",
    "print (sys.path)\n",
    "\n",
    "import util_rasters\n",
    "import util_vectors\n",
    "import util_training\n",
    "# from image_sample_generator import ImageSampleGenerator\n",
    "import util_imagery\n",
    "import util_workflow\n",
    "import util_chips\n",
    "# from batch_generator import BatchGenerator\n",
    "import util_scoring\n",
    "import util_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "data_root='/data/phase_iv/'\n",
    "place = 'kozhikode'\n",
    "data_path=data_root+place+'/'\n",
    "\n",
    "resolution = 5  # Lx:15 S2:10\n",
    "\n",
    "# tiling\n",
    "tile_resolution = resolution\n",
    "tile_size = 256\n",
    "tile_pad = 32\n",
    "\n",
    "# misc\n",
    "s2_bands=['blue','green','red','nir','swir1','swir2','alpha']; suffix='BGRNS1S2A'  # S2, Lx\n",
    "\n",
    "# ground truth source: aue, aue+osm, aue+osm2\n",
    "label_suffix = 'aue'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (place, place.title()) # capitalized version of place name\n",
    "place_title = place.title()\n",
    "place_shapefile = data_path+place_title+\"_studyAreaEPSG4326.shp\"\n",
    "\n",
    "util_vectors.info_studyareas(data_path, place)\n",
    "\n",
    "shape = util_vectors.load_shape(place_shapefile)\n",
    "polygon = shape['geometry']['coordinates']\n",
    "#print polygon\n",
    "#pprint(shape)\n",
    "place_bbox = shape['bbox']\n",
    "#print bbox\n",
    "\n",
    "# using Albers projection\n",
    "lonlat_crs = cartopy.crs.PlateCarree()\n",
    "clat, clon = (place_bbox[0]+place_bbox[2])/2.0, (place_bbox[1]+place_bbox[3])/2.0\n",
    "print (\"center co-ordinates\", clat, clon)\n",
    "albers = cartopy.crs.AlbersEqualArea(central_latitude=clat, central_longitude=clon)\n",
    "\n",
    "# visualize Study Region\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = plt.subplot(projection=albers) # Specify projection of the map here\n",
    "shp = shapely.geometry.shape(shape['geometry'])\n",
    "ax.add_geometries([shp], lonlat_crs)\n",
    "ax.set_extent((place_bbox[0], place_bbox[2], place_bbox[1], place_bbox[3]), crs=lonlat_crs)\n",
    "ax.gridlines(crs=lonlat_crs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping through each tile for all the available scenes, implementing weighted composite per tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_threshold = 0.3\n",
    "test_type = 2\n",
    "\n",
    "for tiles in range(2117):\n",
    "    tile_no = str(tiles).zfill(5)\n",
    "    lulc_list = !find \"/data/phase_iv/scenes/kozhikode/post18monsoon\" -iname {'*_tile*'+tile_no+'_lulc.tif'} -type f\n",
    "    score_list = !find \"/data/phase_iv/scenes/kozhikode/post18monsoon\" -iname {'*_tile*'+str(tile_no)+'_cloudscore.tif'} -type f\n",
    "    pred_list = !find \"/data/phase_iv/scenes/kozhikode/post18monsoon\" -iname {'*_tile*'+str(tile_no)+'_pred.tif'} -type f\n",
    "    \n",
    "    lulcs, scores, preds = util_mapping.prep_lulc_derivation_arrays(lulc_list, score_list, pred_list,3)\n",
    "    \n",
    "    if test_type == 1: \n",
    "#         simple weighted\n",
    "        lulc_derived = util_mapping.derive_lulc_map_predweighted_simple(lulcs, scores, preds, threshold=cloud_threshold)\n",
    "        type = \"predW\"\n",
    "    elif test_type == 2:\n",
    "#         scaled weighted \n",
    "        lulc_derived = util_mapping.derive_lulc_map_predweighted_scaled(lulcs, scores, preds, threshold=cloud_threshold)\n",
    "        type = \"predScl\"\n",
    "\n",
    "    img, geo, prj, cols, rows = util_rasters.load_geotiff(lulc_list[0],dtype='uint8')\n",
    "\n",
    "    lulc_derived_path = '/data/phase_iv/maps/kozhikode/post18monsoon/kozhikode_'+type+'_cld_'+str(cloud_threshold)+'_tile_'+tile_no+'.tif'\n",
    "\n",
    "    util_rasters.write_1band_geotiff(lulc_derived_path, lulc_derived, geo, prj)\n",
    "    \n",
    "    if tiles%100==0:\n",
    "        print(tiles)\n",
    "        print(lulc_derived_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and cropping mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zfill = 5\n",
    "qmarks = '?????'[0:zfill]\n",
    "\n",
    "path_template = '/data/phase_iv/maps/kozhikode/post18monsoon/kozhikode_'+type+'_cld_'+str(cloud_threshold)+'_tile_'+qmarks+'.tif'\n",
    "path_destination = '/data/phase_iv/maps/kozhikode/final_composites/post18monsoon/kozhikode_'+type+'_cld_'+str(cloud_threshold)+'_complete_post18.tif'\n",
    "!gdal_merge.py -n 255 -a_nodata 255 -o {path_destination} {path_template}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_rasters.crop_maps(place_shapefile, [path_destination])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Moved to code base* - Checking uniformity between, lulc, score and pred tiffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_lulc_derivation_arrays(lulc_paths, score_paths, pred_paths, num_cats):\n",
    "    assert len(lulc_paths)==len(score_paths)\n",
    "    assert len(lulc_paths)==len(pred_paths)\n",
    "    img, geo, prj, cols, rows = util_rasters.load_geotiff(lulc_paths[0],dtype='uint8')\n",
    "    assert img.ndim==2\n",
    "    lulcs = np.zeros((len(lulc_paths),)+img.shape, dtype='uint8')\n",
    "    scores = np.zeros((len(lulc_paths),)+img.shape, dtype='float32')\n",
    "    preds = np.zeros((len(lulc_paths),num_cats)+img.shape, dtype='float32')\n",
    "    for i in range(len(lulc_paths)):\n",
    "        lulc_img, lulc_geo, lulc_prj, lulc_cols, lulc_rows = util_rasters.load_geotiff(lulc_paths[i],dtype='uint8')\n",
    "        assert img.shape==lulc_img.shape\n",
    "        assert geo==lulc_geo\n",
    "        assert prj==lulc_prj\n",
    "        assert cols==lulc_cols\n",
    "        assert rows==lulc_rows\n",
    "        scores_img, scores_geo, scores_prj, scores_cols, scores_rows = util_rasters.load_geotiff(score_paths[0],dtype='float32')\n",
    "        assert img.shape==scores_img.shape\n",
    "        assert geo==scores_geo\n",
    "        assert prj==scores_prj\n",
    "        assert cols==scores_cols\n",
    "        assert rows==scores_rows\n",
    "        preds_img, preds_geo, preds_prj, preds_cols, preds_rows = util_rasters.load_geotiff(pred_paths[i],dtype='float32')\n",
    "        assert geo==preds_geo\n",
    "        assert prj==preds_prj\n",
    "        assert cols==preds_cols\n",
    "        assert rows==preds_rows\n",
    "        lulcs[i]=lulc_img\n",
    "        scores[i]=scores_img\n",
    "        preds[i]=preds_img\n",
    "    return lulcs, scores, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Moved to code base* - Create Prediction-Weighted Composite \n",
    "#### (simple sum, includes cloudscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_lulc_map_predweighted_simple(lulcs, scores, preds, categories=[0,1,2], threshold=0.3, stretch=False):\n",
    "\n",
    "    array_shape = lulcs[0].shape\n",
    "    cats = list(categories)\n",
    "    cats.append(255)\n",
    "    votes = np.zeros(((len(cats),)+array_shape), dtype='float32')\n",
    "    valid_masks = (scores<=threshold)\n",
    "\n",
    "    if stretch:\n",
    "        reverse_scores = np.subtract(np.ones(scores.shape, dtype='float32'), np.divide(scores, threshold))\n",
    "    else:\n",
    "        reverse_scores = np.subtract(np.ones(scores.shape, dtype='float32'), scores) # 1 - scores\n",
    "\n",
    "    for i in range(len(cats)):\n",
    "        c = cats[i]\n",
    "        cat_masks = (lulcs==c)\n",
    "        full_masks = (cat_masks & valid_masks)\n",
    "        if i < 3:\n",
    "            pred_disag = preds[:,i] \n",
    "            votes_stack = np.multiply(full_masks, pred_disag, reverse_scores)\n",
    "        else:\n",
    "            pred_disag = 1 \n",
    "            votes_stack = np.multiply(full_masks, pred_disag, reverse_scores)\n",
    "        votes[i] = np.sum(votes_stack, axis=0)\n",
    "\n",
    "    cat_votes = np.sum(votes[:-1], axis=0)\n",
    "    nodata_mask = (cat_votes==0)\n",
    "    winner_indices = np.argmax(votes[:-1], axis=0)\n",
    "\n",
    "    lulc_derived = np.zeros(array_shape, dtype='uint8')\n",
    "\n",
    "    for i in range(len(cats)):\n",
    "        mask = (winner_indices==i)\n",
    "        lulc_derived[mask] = cats[i]\n",
    "    lulc_derived[nodata_mask]=255\n",
    "    return lulc_derived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Moved to code base* - Create Prediction-Weighted Composite \n",
    "#### (brookie method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_scale(val,in_a,in_b,out_a,out_b,scale=1.0):\n",
    "    mn,mx=sorted([in_a,in_b])\n",
    "    val=np.clip(val,mn,mx)\n",
    "    slope=(out_b-out_a)/(in_b-in_a)\n",
    "    return scale * ( (val-in_a)*slope + out_a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud_scale(\n",
    "        score,\n",
    "        min_score=0.05,\n",
    "        max_score=0.9,\n",
    "        min_value=0.1,\n",
    "        max_value=1.1,\n",
    "        scale=1.0):\n",
    "    return linear_scale(\n",
    "        val=score,\n",
    "        in_a=max_score,\n",
    "        in_b=min_score,\n",
    "        out_a=min_value,\n",
    "        out_b=max_value,\n",
    "        scale=scale)\n",
    "\n",
    "def pred_scale(\n",
    "        pred,\n",
    "        min_pred=0.6,\n",
    "        max_pred=0.99,\n",
    "        min_value=0.7,\n",
    "        max_value=1.0,\n",
    "        scale=1.0):\n",
    "    return linear_scale(\n",
    "        val=pred,\n",
    "        in_a=min_pred,\n",
    "        in_b=max_pred,\n",
    "        out_a=min_value,\n",
    "        out_b=max_value,\n",
    "        scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(pred,cloud_score):\n",
    "    return pred_scale(pred)+cloud_scale(cloud_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(cpc,classes=range(4)):\n",
    "    scores={}\n",
    "    for c in classes:\n",
    "        scores[c]=0\n",
    "        pred_clouds=cpc[cpc[:,0]==c][:,1:]\n",
    "        for pc in pred_clouds:\n",
    "            scores[c]+=weight(*pc)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_lulc_map_predweighted_scaled(lulcs, scores, preds, categories=[0,1,2], threshold=0.3, stretch=False):\n",
    "\n",
    "    array_shape = lulcs[0].shape\n",
    "    cats = list(categories)\n",
    "    cats.append(255)\n",
    "    votes = np.zeros(((len(cats),)+array_shape), dtype='float32')\n",
    "    valid_masks = (scores<=threshold)\n",
    "\n",
    "    if stretch:\n",
    "        reverse_scores = np.subtract(np.ones(scores.shape, dtype='float32'), np.divide(scores, threshold))\n",
    "    else:\n",
    "        reverse_scores = np.subtract(np.ones(scores.shape, dtype='float32'), scores) # 1 - scores\n",
    "\n",
    "    for i in range(len(cats)):\n",
    "        c = cats[i]\n",
    "        cat_masks = (lulcs==c)\n",
    "        full_masks = (cat_masks & valid_masks)\n",
    "        if i < 3:\n",
    "            pred_disag = preds[:,i] \n",
    "            scaled_weight = weight(pred_disag, scores)\n",
    "            votes_stack = np.multiply(full_masks, scaled_weight)\n",
    "        else:\n",
    "            pred_disag = 1 \n",
    "            scaled_weight = weight(pred_disag, scores)\n",
    "            votes_stack = np.multiply(full_masks, scaled_weight)\n",
    "        votes[i] = np.sum(votes_stack, axis=0)\n",
    "\n",
    "    cat_votes = np.sum(votes[:-1], axis=0)\n",
    "    nodata_mask = (cat_votes==0)\n",
    "    winner_indices = np.argmax(votes[:-1], axis=0)\n",
    "\n",
    "    lulc_derived = np.zeros(array_shape, dtype='uint8')\n",
    "\n",
    "    for i in range(len(cats)):\n",
    "        mask = (winner_indices==i)\n",
    "        lulc_derived[mask] = cats[i]\n",
    "    lulc_derived[nodata_mask]=255\n",
    "    return lulc_derived"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geoml] *",
   "language": "python",
   "name": "conda-env-geoml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
