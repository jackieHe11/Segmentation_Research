{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development: Train 3-Category Classifier\n",
    "Use the latest and greatest model structures, parameters, workflow (ie class weighting, learning fast & slow, etc) to train a 3-category classifier using only the non-road training samples.\n",
    "\n",
    "This is the latest version as of the start of Phase IV, meant to leverage the previous changes in order to utilize the `fit_generator`.\n",
    "\n",
    "Date: 2019-02-01  \n",
    "Author: Peter Kerins  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements\n",
    "(may be over-inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    u'bbox': [-91.512974, 36.970298, -87.019935, 42.508302],\n",
      "    u'id': 85688697,\n",
      "    u'name': u'Illinois',\n",
      "    u'path': u'continent:north-america_country:united-states_region:illinois',\n",
      "    u'placetype': u'region',\n",
      "    u'slug': u'north-america_united-states_illinois'\n",
      "  }\n",
      "]\n",
      "['', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/home/Peter.Kerins/.local/lib/python2.7/site-packages', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/PILcompat', '/usr/lib/python2.7/dist-packages/gtk-2.0', '/usr/local/lib/python2.7/dist-packages/IPython/extensions', '/home/Peter.Kerins/.ipython', '/home/Peter.Kerins/UrbanLandUse/utils']\n"
     ]
    }
   ],
   "source": [
    "# typical, comprehensive imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import itertools\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "#\n",
    "import numpy as np\n",
    "import shapely\n",
    "import cartopy\n",
    "import geojson\n",
    "import fiona\n",
    "import gdal\n",
    "import h5py\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import ogr, gdal\n",
    "from keras.models import load_model\n",
    "import math\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Add, Lambda\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "\n",
    "import collections\n",
    "from pprint import pprint\n",
    "\n",
    "import descarteslabs as dl\n",
    "print dl.places.find('illinois') ## TEST\n",
    "\n",
    "ULU_REPO = os.environ[\"ULU_REPO\"]\n",
    "sys.path.append(ULU_REPO+'/utils')\n",
    "print sys.path\n",
    "\n",
    "import util_descartes\n",
    "import util_ml\n",
    "import util_rasters\n",
    "import util_vectors\n",
    "import util_workflow\n",
    "import util_chips\n",
    "import util_training\n",
    "import util_network\n",
    "import util_scoring\n",
    "from batch_generator import BatchGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root='/data/phase_iv/'\n",
    "\n",
    "tile_resolution = 5\n",
    "tile_size = 256\n",
    "tile_pad = 32\n",
    "resolution=tile_resolution  # Lx:15 S2:10\n",
    "\n",
    "processing_level = None\n",
    "source = 's2'\n",
    "#image_suffix = 'E'\n",
    "\n",
    "s2_bands=['blue','green','red','nir','swir1','swir2','alpha']; s2_suffix='BGRNS1S2A'  # S2, Lx\n",
    "\n",
    "s1_bands=['vv','vh']; s1_suffix='VVVH'  \n",
    "\n",
    "resampling='bilinear'\n",
    "processing = None\n",
    "\n",
    "label_suffix = 'aue'\n",
    "label_lot = '0'\n",
    "\n",
    "# can do much more sophisticated filtering than this, but fine for demonstration\n",
    "place_images = {}\n",
    "place_images['sitapur']=['E']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify training data & training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 17\n",
    "\n",
    "# bands stuff outdated! needs to be reconciled with catalog filtering\n",
    "# will ignore for the moment since this is a bigger fix...\n",
    "# haven't done any examples yet incorporating additional chips beyond s2\n",
    "# into construction of a training sample\n",
    "bands_vir=s2_bands[:-1]\n",
    "bands_sar=None\n",
    "bands_ndvi=None\n",
    "bands_ndbi=None\n",
    "bands_osm=None\n",
    "\n",
    "# this can get updated when cloudmasking is added\n",
    "haze_removal = False\n",
    "\n",
    "epochs = 500 # this is fine, if irrelevant\n",
    "batch_size = 128\n",
    "balancing = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vir 6\n"
     ]
    }
   ],
   "source": [
    "# needs to be updated completely; bands stuff doesn't make sense right now\n",
    "stack_label, feature_count = util_workflow.build_stack_label(\n",
    "        bands_vir=bands_vir,\n",
    "        bands_sar=bands_sar,\n",
    "        bands_ndvi=bands_ndvi,\n",
    "        bands_ndbi=bands_ndbi,\n",
    "        bands_osm=bands_osm,)\n",
    "print stack_label, feature_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify data of interest\n",
    "Load and manipulate the catalog to specify which samples are of interest for this training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8147584\n"
     ]
    }
   ],
   "source": [
    "df = util_chips.load_catalog()\n",
    "print len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>image</th>\n",
       "      <th>processing</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hindupur</td>\n",
       "      <td>G</td>\n",
       "      <td>surface</td>\n",
       "      <td>120304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jaipur</td>\n",
       "      <td>F</td>\n",
       "      <td>surface</td>\n",
       "      <td>318851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mexico-city</td>\n",
       "      <td>A</td>\n",
       "      <td>none</td>\n",
       "      <td>846464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mexico-city</td>\n",
       "      <td>B</td>\n",
       "      <td>none</td>\n",
       "      <td>846464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mexico-city</td>\n",
       "      <td>C</td>\n",
       "      <td>none</td>\n",
       "      <td>846464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>parbhani</td>\n",
       "      <td>H</td>\n",
       "      <td>none</td>\n",
       "      <td>231665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>parbhani</td>\n",
       "      <td>H</td>\n",
       "      <td>surface</td>\n",
       "      <td>231665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>parbhani</td>\n",
       "      <td>I</td>\n",
       "      <td>none</td>\n",
       "      <td>231665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>parbhani</td>\n",
       "      <td>J</td>\n",
       "      <td>none</td>\n",
       "      <td>231665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>parbhani</td>\n",
       "      <td>K</td>\n",
       "      <td>none</td>\n",
       "      <td>231665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>parbhani</td>\n",
       "      <td>L</td>\n",
       "      <td>none</td>\n",
       "      <td>231665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>singrauli</td>\n",
       "      <td>D</td>\n",
       "      <td>surface</td>\n",
       "      <td>188715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>AS</td>\n",
       "      <td>surface</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>BB</td>\n",
       "      <td>surface</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>E</td>\n",
       "      <td>none</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>F</td>\n",
       "      <td>none</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>G</td>\n",
       "      <td>none</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>H</td>\n",
       "      <td>none</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>Q</td>\n",
       "      <td>none</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>Q</td>\n",
       "      <td>surface</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>R</td>\n",
       "      <td>none</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>R</td>\n",
       "      <td>surface</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>S</td>\n",
       "      <td>none</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>T</td>\n",
       "      <td>none</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>T</td>\n",
       "      <td>surface</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>U</td>\n",
       "      <td>none</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>U</td>\n",
       "      <td>surface</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>V</td>\n",
       "      <td>none</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>V</td>\n",
       "      <td>surface</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>W</td>\n",
       "      <td>none</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sitapur</td>\n",
       "      <td>W</td>\n",
       "      <td>surface</td>\n",
       "      <td>172551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>vijayawada</td>\n",
       "      <td>C</td>\n",
       "      <td>surface</td>\n",
       "      <td>311863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           city image processing   count\n",
       "0      hindupur     G    surface  120304\n",
       "1        jaipur     F    surface  318851\n",
       "2   mexico-city     A       none  846464\n",
       "3   mexico-city     B       none  846464\n",
       "4   mexico-city     C       none  846464\n",
       "5      parbhani     H       none  231665\n",
       "6      parbhani     H    surface  231665\n",
       "7      parbhani     I       none  231665\n",
       "8      parbhani     J       none  231665\n",
       "9      parbhani     K       none  231665\n",
       "10     parbhani     L       none  231665\n",
       "11    singrauli     D    surface  188715\n",
       "12      sitapur    AS    surface  172551\n",
       "13      sitapur    BB    surface  172551\n",
       "14      sitapur     E       none  172551\n",
       "15      sitapur     F       none  172551\n",
       "16      sitapur     G       none  172551\n",
       "17      sitapur     H       none  172551\n",
       "18      sitapur     Q       none  172551\n",
       "19      sitapur     Q    surface  172551\n",
       "20      sitapur     R       none  172551\n",
       "21      sitapur     R    surface  172551\n",
       "22      sitapur     S       none  172551\n",
       "23      sitapur     T       none  172551\n",
       "24      sitapur     T    surface  172551\n",
       "25      sitapur     U       none  172551\n",
       "26      sitapur     U    surface  172551\n",
       "27      sitapur     V       none  172551\n",
       "28      sitapur     V    surface  172551\n",
       "29      sitapur     W       none  172551\n",
       "30      sitapur     W    surface  172551\n",
       "31   vijayawada     C    surface  311863"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['city','image','processing']).size().reset_index().rename(columns={0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['city']=='sitapur') & (df['image']=='V')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = pd.Series(data=np.zeros(len(df.index),dtype='uint8'), index=range(len(df)), dtype='uint8')\n",
    "\n",
    "for place,image_list in place_images.iteritems():\n",
    "    for image in image_list:\n",
    "        mask |= (df['city']==place) & (df['image']==image)\n",
    "\n",
    "# straight away remove road samples\n",
    "mask &= (df['lulc']!=6)\n",
    "\n",
    "# filter others according to specifications\n",
    "mask &= (df['gt_type']==label_suffix)\n",
    "mask &= (df['gt_lot']==int(label_lot))\n",
    "mask &= (df['source']==source)\n",
    "mask &= (df['resolution']==int(resolution))\n",
    "mask &= (df['resampling']==resampling)\n",
    "mask &= (df['processing']==str(processing).lower())\n",
    "\n",
    "print np.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here for example we will just exclude all roads samples\n",
    "df = df[mask]\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data (ie catalogs) into training and validation tranches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(util_chips)\n",
    "place_locales = util_chips.apportion_locales(df)\n",
    "print place_locales\n",
    "print len(place_locales['sitapur'][0]), len(place_locales['sitapur'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t, df_v = util_chips.mask_locales(df, place_locales)\n",
    "print len(df_t), len(df_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect dataframes for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print util_training.calc_category_counts(df_t,remapping=None), len(df_t)\n",
    "print util_training.calc_category_counts(df_v,remapping=None), len(df_v)\n",
    "print\n",
    "print util_training.calc_category_counts(df,remapping=None), len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate class weighting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_weights =  util_training.generate_category_weights(df,remapping='standard',log=False,mu=1.0,max_score=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print category_weights.items()\n",
    "weights = list(zip(*category_weights.items())[1])\n",
    "print weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use weights to make weighted categorical crossentropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = util_training.make_loss_function_wcc(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name and briefly describe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = '3cat_Sit_gen-test'\n",
    "notes = 'new architecture: sitapur E only, 3cat 5m bilinear, no processing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DEFAULT:\",K.image_data_format())\n",
    "K.set_image_data_format('channels_first')\n",
    "print(\"UPDATED:\",K.image_data_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create actual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hardcoded params\n",
    "network=util_network.build_model(util_network.doubleres_block,input_shape=(6,17,17),output_nodes=3)\n",
    "util_network.compile_network(network, loss, LR=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create generators to provide training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_t = BatchGenerator(df_t,remapping='3cat',look_window=window,batch_size=batch_size,one_hot=3)\n",
    "generator_v = BatchGenerator(df_v,remapping='3cat',look_window=window,batch_size=batch_size,one_hot=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create callback functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_label='weights_fast'\n",
    "callbacks, weights_path = util_training.create_callbacks(data_root, model_id, weights_label=weights_label, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conduct training using `fit_generator` and visualize progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train fast\n",
    "#history_fast = network.fit(X_train, Y_t_cat, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, Y_v_cat), shuffle=True,callbacks=callbacks)\n",
    "#docs: fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None,\n",
    "                    #class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
    "history_fast = network.fit_generator(generator_t, epochs=epochs, callbacks=callbacks, steps_per_epoch=generator_t.steps,\n",
    "                                    validation_data=generator_v, validation_steps=generator_v.steps,\n",
    "                                    shuffle=True,use_multiprocessing=True,max_queue_size=40,workers=64,)\n",
    "\n",
    "plt.plot(history_fast.history['val_acc'])\n",
    "plt.show()\n",
    "plt.plot(history_fast.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild model and train slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hardcoded params\n",
    "network=util_network.build_model(util_network.doubleres_block,input_shape=(6,17,17),output_nodes=3)\n",
    "# load weights from fast learning\n",
    "network.load_weights(weights_path)\n",
    "\n",
    "util_network.compile_network(network, loss, LR=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create generators to provide training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_t.reset()\n",
    "generator_v.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create callback functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_label='weights_slow'\n",
    "callbacks, weights_path = util_training.create_callbacks(data_root, model_id, weights_label=weights_label, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conduct training using `fit_generator` and visualize progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_slow = network.fit_generator(generator_t, epochs=epochs, callbacks=callbacks, steps_per_epoch=generator_t.steps,\n",
    "                                    validation_data=generator_v, validation_steps=generator_v.steps,\n",
    "                                    shuffle=True,use_multiprocessing=True,max_queue_size=40,workers=64,)\n",
    "\n",
    "plt.plot(history_slow.history['val_acc'])\n",
    "plt.show()\n",
    "plt.plot(history_slow.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model to training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_t.reset()\n",
    "#predict_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
    "predictions_t = network.predict_generator(generator_t, steps=generator_t.steps, verbose=1,\n",
    "                  use_multiprocessing=True, max_queue_size=40, workers=64,)\n",
    "print predictions_t.shape\n",
    "\n",
    "generator_v.reset()\n",
    "#predict_generator(generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
    "predictions_v = network.predict_generator(generator_v, steps=generator_v.steps, verbose=1,\n",
    "                  use_multiprocessing=True, max_queue_size=40, workers=64,)\n",
    "print predictions_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat_t = predictions_t.argmax(axis=-1)\n",
    "print Yhat_t.shape\n",
    "Yhat_v = predictions_v.argmax(axis=-1)\n",
    "print Yhat_v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract corresponding _actual_ ground-truth values directly from catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_t = generator_t.get_label_series().values\n",
    "print Y_t.shape\n",
    "Y_v = generator_v.get_label_series().values\n",
    "print Y_v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate typical scoring information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"evaluate training\"\n",
    "# hardcoded categories\n",
    "categories=[0,1,2]\n",
    "train_confusion = util_scoring.calc_confusion(Yhat_t,Y_t,categories)\n",
    "train_recalls, train_precisions, train_accuracy = util_scoring.calc_confusion_details(train_confusion)\n",
    "\n",
    "# Calculate f-score\n",
    "beta = 2\n",
    "train_f_score = (beta**2 + 1) * train_precisions * train_recalls / ( (beta**2 * train_precisions) + train_recalls )\n",
    "train_f_score_open = train_f_score[0] \n",
    "train_f_score_nonres = train_f_score[1]  \n",
    "train_f_score_res = train_f_score[2]  \n",
    "train_f_score_roads = None#train_f_score[3]  \n",
    "train_f_score_average = np.mean(train_f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"evaluate validation\"\n",
    "valid_confusion = util_scoring.calc_confusion(Yhat_v,Y_v,categories)\n",
    "valid_recalls, valid_precisions, valid_accuracy = util_scoring.calc_confusion_details(valid_confusion)\n",
    "\n",
    "# Calculate f-score\n",
    "valid_f_score = (beta**2 + 1) * valid_precisions * valid_recalls / ( (beta**2 * valid_precisions) + valid_recalls )\n",
    "valid_f_score_open = valid_f_score[0] \n",
    "valid_f_score_nonres = valid_f_score[1] \n",
    "valid_f_score_res = valid_f_score[2] \n",
    "valid_f_score_roads = None# valid_f_score[3] \n",
    "valid_f_score_average = np.mean(valid_f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanding lists to match expected model_record stuff\n",
    "train_recalls_expanded = [train_recalls[0],train_recalls[1],train_recalls[2],None]\n",
    "valid_recalls_expanded = [valid_recalls[0],valid_recalls[1],valid_recalls[2],None]\n",
    "train_precisions_expanded = [train_precisions[0],train_precisions[1],train_precisions[2],None]\n",
    "valid_precisions_expanded = [valid_precisions[0],valid_precisions[1],valid_precisions[2],None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record experiment configuration and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(util_scoring)\n",
    "#scaler_filename = data_root+'models/'+model_id+'_scaler.pkl'\n",
    "#model_filename  = data_root+'models/'+model_id+'_SVM.pkl'\n",
    "place_locales_filename = data_root+'models/'+model_id+'_place_locales.pkl'\n",
    "category_weights_filename = data_root+'models/'+model_id+'_category_weights.pkl'\n",
    "network_filename = data_root+'models/'+model_id+'.hd5'\n",
    "\n",
    "if os.path.exists(network_filename):\n",
    "    print 'Aborting all pickle operations: file already exists at specified path ('+network_filename+')'\n",
    "elif os.path.exists(category_weights_filename):\n",
    "    print 'Aborting all pickle operations: file already exists at specified path ('+category_weights_filename+')'\n",
    "else:\n",
    "    print network_filename\n",
    "    pickle.dump(place_locales, open(place_locales_filename, 'wb'))\n",
    "    pickle.dump(category_weights, open(category_weights_filename, 'wb'))\n",
    "    network.save(network_filename)\n",
    "    # tracking only occurs if all saves are successful\n",
    "    util_scoring.record_model_creation(\n",
    "        model_id, notes, place_images, label_suffix+label_lot, resolution, stack_label, feature_count, \n",
    "        window, generator_t.remapping, balancing, \n",
    "        network.get_config(), epochs, batch_size,\n",
    "        train_confusion, train_recalls_expanded, train_precisions_expanded, train_accuracy,\n",
    "        train_f_score_open, train_f_score_nonres, train_f_score_res, train_f_score_roads, train_f_score_average,\n",
    "        valid_confusion, valid_recalls_expanded, valid_precisions_expanded, valid_accuracy,\n",
    "        valid_f_score_open, valid_f_score_nonres, valid_f_score_res, valid_f_score_roads, valid_f_score_average,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
